<p align="center">
  <a href="https://suajornadadedados.com.br/"><img src="https://github.com/lvgalvao/data-engineering-roadmap/raw/main/pics/logo.png" alt="Jornada de Dados"></a>
</p>
<p align="center">
  <em>Nossa missÃ£o Ã© fornecer o melhor ensino em engenharia de dados</em>
</p>

<p align="center">
  <img src="img/SeUZPWKQ.png" alt="Data Pipeline Bitcoin - ETL com Python e Databricks" width="800">
</p>

---

# ğŸ’° **Data Pipeline: ExtraÃ§Ã£o de Dados Bitcoin com ETL em Python**

## ğŸ“‹ **Sobre o Projeto**

Este projeto faz parte de um **workshop gratuito de Data Engineering para Iniciantes**, realizado no dia **16/12/2025 Ã s 19h30**, que pode ser assistido aqui: [YouTube Live](https://www.youtube.com/live/pFJCL1S3Zj8)

O projeto Ã© focado na criaÃ§Ã£o de pipelines de dados **ETL (Extract, Transform, Load)** do zero. O objetivo Ã© construir um programa completo que consome dados de uma **API** (Coinbase), organiza esses dados e armazena em diferentes formatos (JSON, Parquet, Delta Lake), alÃ©m de criar visualizaÃ§Ãµes e agentes de IA no Databricks.

### ğŸ¯ **O que vocÃª vai aprender:**

- âœ… O que Ã© uma API e como consumi-la usando Python
- âœ… O processo completo de ETL (extraÃ§Ã£o, transformaÃ§Ã£o e carga)
- âœ… DiferenÃ§as entre formatos de armazenamento (Row-based vs Columnar)
- âœ… Como automatizar a execuÃ§Ã£o do pipeline para coleta contÃ­nua
- âœ… CriaÃ§Ã£o de dashboards no Databricks
- âœ… ImplementaÃ§Ã£o de agentes de IA para anÃ¡lise de dados

### ğŸš€ **Resultado Final**

Ao final do projeto, vocÃª terÃ¡:
- âœ… Um programa funcional de pipeline ETL
- âœ… Dashboard interativo no Databricks
- âœ… Agente de IA para monitoramento e anÃ¡lise de preÃ§os

---

## ğŸ“Š **Esquema do Projeto**

Visualize a arquitetura completa do projeto: [app.excalidraw.com](https://app.excalidraw.com/s/8pvW6zbNUnD/9zZctm3OR9f)

---

## ğŸ¯ **Overview do Projeto**

### **Objetivo Principal**

Desenvolver um pipeline ETL automatizado para consumir dados da **API da Coinbase** e armazenar informaÃ§Ãµes sobre o preÃ§o da Bitcoin de forma estruturada e escalÃ¡vel.

### **Etapas do Projeto**

#### 1. **ExtraÃ§Ã£o (E)**

- Utilizar a **API da Coinbase** para obter o preÃ§o atual da Bitcoin
- Implementar tratamento de erros e retry logic
- Coleta de dados em tempo real

#### 2. **Carga (L)**

- Armazenar dados brutos em arquivo **JSON** (formato legÃ­vel)
- PersistÃªncia inicial dos dados extraÃ­dos

#### 3. **TransformaÃ§Ã£o (T)**

- Selecionar apenas as informaÃ§Ãµes relevantes: preÃ§o da Bitcoin, horÃ¡rio da consulta e moeda de referÃªncia (USD)
- Organizar os dados utilizando **PySpark**
- Aplicar transformaÃ§Ãµes e validaÃ§Ãµes de dados
- Converter para formatos otimizados (Parquet, Delta Lake)

#### 4. **VisualizaÃ§Ã£o**

- Criar um dashboard usando o **Databricks** para monitorar os preÃ§os em tempo real
- GrÃ¡ficos interativos e anÃ¡lises visuais

#### 5. **Agente de IA**

- Criar um agente de IA usando o **Databricks** para monitorar os preÃ§os em tempo real
- AnÃ¡lises automatizadas e insights inteligentes

---

## ğŸ”§ **Stack TecnolÃ³gico**

Neste projeto, utilizamos bibliotecas essenciais para qualquer Engenheiro de Dados. Entenda o porquÃª de cada uma:

### **1. Requests** (`requests`)

- **O que faz**: Ã‰ a biblioteca mais popular do Python para fazer requisiÃ§Ãµes HTTP
- **Por que usamos**: Para "conversar" com a API da Coinbase. Ela envia o pedido (GET) e recebe a resposta com os dados do Bitcoin. Ã‰ a porta de entrada dos dados no nosso pipeline

### **2. Pandas** (`pandas`)

- **O que faz**: Biblioteca fundamental para manipulaÃ§Ã£o e anÃ¡lise de dados em Python
- **Por que usamos**: Para estruturar e transformar os dados extraÃ­dos da API antes de salvÃ¡-los em diferentes formatos

### **3. PySpark** (`pyspark`)

- **O que faz**: API Python para Apache Spark, framework de processamento distribuÃ­do
- **Por que usamos**: Para processar grandes volumes de dados de forma distribuÃ­da e preparar os dados para o Databricks

### **4. PyArrow** (`pyarrow`)

- **O que faz**: Biblioteca para processamento de dados colunares e intercÃ¢mbio entre sistemas
- **Por que usamos**: O Pandas precisa dele para salvar arquivos no formato **Parquet**. O Parquet Ã© crucial em Big Data porque comprime os dados e permite leitura rÃ¡pida, sendo o formato nativo de Data Lakes

### **5. Databricks**

- **O que faz**: Plataforma unificada de anÃ¡lise de dados baseada em Spark
- **Por que usamos**: Para criar dashboards, agentes de IA e processar dados em escala empresarial

---

## ğŸ“š **Por Que Usar Databricks e PySpark?**

Embora este projeto rode localmente, ele prepara vocÃª para ambientes de Big Data como o **Databricks**. No mundo corporativo, lidamos com terabytes de dados.

- **PySpark**: Ã‰ a API Python para o Apache Spark. Ao contrÃ¡rio do Pandas, que roda em uma Ãºnica mÃ¡quina, o Spark processa dados de forma distribuÃ­da em um cluster de computadores
- **Databricks**: Ã‰ uma plataforma unificada de anÃ¡lise de dados baseada em Spark. Ela facilita a criaÃ§Ã£o de Data Lakes e Data Warehouses modernos (Lakehouse)

### **ComparaÃ§Ã£o de Formatos de Armazenamento**

#### ğŸ“„ **Arquivos de Texto (Text Files)**

SÃ£o arquivos legÃ­veis por humanos, cujo conteÃºdo Ã© texto puro (ASCII / UTF-8).

**Exemplos clÃ¡ssicos:**

- CSV
- JSON
- TXT
- XML
- YAML

**CaracterÃ­sticas:**

- âœ… DÃ¡ para abrir no bloco de notas
- âœ… FÃ¡cil de debugar
- âŒ Maior tamanho
- âŒ Leitura e escrita mais lentas em grandes volumes

#### ğŸ§± **Arquivos BinÃ¡rios (Binary Files)**

SÃ£o arquivos nÃ£o legÃ­veis diretamente por humanos, otimizados para mÃ¡quinas.

**Exemplos:**

- JPEG / PNG / MP3 / MP4
- **Parquet**
- ORC
- Avro
- PDF (em geral)
- ExecutÃ¡veis (.exe)

**CaracterÃ­sticas:**

- âœ… Compactados
- âœ… Estruturados internamente
- âœ… Muito mais eficientes para processamento
- âš ï¸ Exigem um software/biblioteca para leitura

#### ğŸ¯ **E o Parquet?**

ğŸ‘‰ **Sim, Parquet Ã© um arquivo binÃ¡rio.** âœ”ï¸

Mais do que isso:

- Ã‰ um arquivo binÃ¡rio **columnar**
- Otimizado para:
  - Leitura por coluna
  - CompressÃ£o
  - Analytics (Big Data, Data Warehouses, Lakehouse)

**Frase profissional:**
> "O Parquet Ã© um formato binÃ¡rio e columnar, otimizado para processamento analÃ­tico e grandes volumes de dados."

**ExplicaÃ§Ã£o didÃ¡tica:**
> "De forma geral, podemos dividir os arquivos em dois grandes grupos: arquivos de texto, como CSV e JSON, que sÃ£o legÃ­veis por humanos; e arquivos binÃ¡rios, como imagens, vÃ­deos e formatos analÃ­ticos como o Parquet, que sÃ£o otimizados para processamento por mÃ¡quinas."

### **ComparaÃ§Ã£o RÃ¡pida**

| Formato | Tipo | LegÃ­vel por humano? |
|---------|------|---------------------|
| CSV | Texto | âœ… |
| JSON | Texto | âœ… |
| TXT | Texto | âœ… |
| Excel (.xls/.xlsx) | BinÃ¡rio | âŒ |
| Parquet | BinÃ¡rio | âŒ |
| JPEG | BinÃ¡rio | âŒ |

---

## ğŸŒ **O Mundo Real: Databricks**

Embora este projeto rode no seu computador, ele foi desenhado para simular o que acontece em grandes empresas que usam **Databricks**.

### **O que Ã© o Databricks?**

O Databricks Ã© uma plataforma de anÃ¡lise de dados baseada em nuvem, criada pelos fundadores do Apache Spark. Ela unifica **Engenharia de Dados**, **CiÃªncia de Dados** e **Machine Learning** em um Ãºnico lugar (Lakehouse).

### **Por que ele Ã© importante?**

1. **Processamento em Escala**: Enquanto o Pandas processa megabytes na sua RAM, o Databricks (via Spark) processa petabytes distribuÃ­dos em centenas de computadores

2. **ColaboraÃ§Ã£o**: Notebooks compartilhados (como o Jupyter) permitem que times trabalhem juntos

3. **Modern Data Stack**: Ele incentiva o uso de **Parquet/Delta Lake** (que estamos simulando aqui) como padrÃ£o de armazenamento, garantindo performance e confiabilidade

Neste workshop, vocÃª estÃ¡ aprendendo os **fundamentos** (ETL, formatos de arquivo, APIs) que sÃ£o exatamente os mesmos usados dentro do Databricks, apenas em menor escala.

---

## ğŸš€ **Como Usar**

### **PrÃ©-requisitos**

- Python 3.8 ou superior
- Conta no Databricks (gratuita para testes)
- Conhecimento bÃ¡sico de Python

### **InstalaÃ§Ã£o**

```bash
# Clone o repositÃ³rio
git clone https://github.com/seu-usuario/pipeline-api-bitcoin-com-databricks.git
cd pipeline-api-bitcoin-com-databricks

# Instale as dependÃªncias
pip install -r requirements.txt
```

### **ExecuÃ§Ã£o**

```bash
# Execute o pipeline
python main.py
```

---

## ğŸ“– **Estrutura do Projeto**

```text
pipeline-api-bitcoin-com-databricks/
â”œâ”€â”€ img/
â”‚   â””â”€â”€ SeUZPWKQ.png          # Imagem de capa do projeto
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract.py            # MÃ³dulo de extraÃ§Ã£o da API
â”‚   â”œâ”€â”€ transform.py          # MÃ³dulo de transformaÃ§Ã£o
â”‚   â””â”€â”€ load.py               # MÃ³dulo de carga
â”œâ”€â”€ notebooks/                # Notebooks do Databricks
â”œâ”€â”€ data/                     # Dados gerados
â”‚   â”œâ”€â”€ raw/                  # Dados brutos (JSON)
â”‚   â””â”€â”€ processed/            # Dados processados (Parquet)
â”œâ”€â”€ requirements.txt          # DependÃªncias do projeto
â””â”€â”€ README.md                 # Este arquivo
```

---

## ğŸ“ **Workshop**

Este projeto foi desenvolvido durante um workshop ao vivo. Assista a gravaÃ§Ã£o completa:

ğŸ”— [YouTube Live - Workshop Data Engineering](https://www.youtube.com/live/pFJCL1S3Zj8)

**Data:** 16/12/2025 Ã s 19h30

---

## ğŸ“ **LicenÃ§a**

Este projeto Ã© parte do conteÃºdo educacional da **Jornada de Dados**.

---

## ğŸ‘¥ **Contato**

- **Website**: [suajornadadedados.com.br](https://suajornadadedados.com.br/)
- **YouTube**: [Canal Jornada de Dados](https://www.youtube.com/@JornadadeDados)

---

<p align="center">
  <em>Desenvolvido com â¤ï¸ pela equipe Jornada de Dados</em>
</p>
