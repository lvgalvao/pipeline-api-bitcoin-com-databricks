<p align="center">
  <a href="https://suajornadadedados.com.br/"><img src="https://github.com/lvgalvao/data-engineering-roadmap/raw/main/pics/logo.png" alt="Jornada de Dados"></a>
</p>
<p align="center">
    <em>Nossa miss√£o √© fornecer o melhor ensino em engenharia de dados</em>
</p>

---

# üí∞ **Data Pipeline: Extra√ß√£o de Dados Bitcoin com ETL em PySpark e Databricks**

## üìã **Sobre o Projeto**

Este projeto faz parte de um **workshop gratuito de Data Engineering para Iniciantes**, realizado no dia **16/12/2025 √†s 19h30**, que pode ser assistido aqui: 

[![Workshop Data Engineering](./img/SeUZPWKQ.png)](https://www.youtube.com/live/pFJCL1S3Zj8)

---

## üéØ **O que vamos construir?**

Neste workshop, voc√™ vai construir um **pipeline ETL completo** que extrai dados de APIs, transforma e armazena em Delta Tables, e cria um dashboard interativo para visualiza√ß√£o.

### **Arquitetura Completa do Projeto**

```mermaid
flowchart TB
    A["üåê API Coinbase<br/><b>Bitcoin USD</b>"] --> E["üì• EXTRACT"]
    B["üåê API CurrencyFreaks<br/><b>USD-BRL Rate</b>"] --> E
    E --> T["üîÑ TRANSFORM<br/>‚Ä¢ Convert USD‚ÜíBRL<br/>‚Ä¢ Add timestamp<br/>‚Ä¢ Structure data"]
    T --> L["üíæ LOAD<br/>Delta Table<br/>Unity Catalog"]
    L --> W["‚öôÔ∏è WORKFLOW<br/>Databricks Jobs<br/>Automa√ß√£o"]
    W --> D["üìä DASHBOARD<br/>Visualiza√ß√µes<br/>M√©tricas em Tempo Real"]
    
    style A fill:#e1f5ff,stroke:#0066cc,stroke-width:2px
    style B fill:#e1f5ff,stroke:#0066cc,stroke-width:2px
    style E fill:#fff4e1,stroke:#ff9900,stroke-width:2px
    style T fill:#ffe1f5,stroke:#cc0066,stroke-width:2px
    style L fill:#e1ffe1,stroke:#00cc66,stroke-width:2px
    style W fill:#f0e1ff,stroke:#9900cc,stroke-width:2px
    style D fill:#ffe1f0,stroke:#cc0099,stroke-width:2px
```

### **Componentes do Projeto:**

1. **üì• EXTRACT**: Extra√ß√£o de dados de 2 APIs (Coinbase e CurrencyFreaks)
2. **üîÑ TRANSFORM**: Convers√£o de moedas e estrutura√ß√£o de dados
3. **üíæ LOAD**: Armazenamento em Delta Table no Unity Catalog
4. **‚öôÔ∏è WORKFLOW**: Automa√ß√£o via Databricks Jobs & Pipelines
5. **üìä DASHBOARD**: Visualiza√ß√£o interativa com m√©tricas e gr√°ficos

---

## üéì **Objetivos da Aula**

### **Objetivo Geral**

Ensinar os **fundamentos pr√°ticos de Data Engineering** atrav√©s da constru√ß√£o de um pipeline ETL completo, desde a extra√ß√£o de dados de APIs at√© a cria√ß√£o de dashboards interativos, utilizando tecnologias modernas e amplamente utilizadas no mercado.

### **Objetivos de Aprendizado**

#### **1. Fundamentos de Python para Data Engineering**
- ‚úÖ **Vari√°veis e tipos de dados** (string, int, float, bool, list, dict)
- ‚úÖ **Fun√ß√µes e m√©todos** (cria√ß√£o, reutiliza√ß√£o, m√©todos de string/list/dict)
- ‚úÖ **Manipula√ß√£o de dados** (estruturas de dados, itera√ß√µes)
- ‚úÖ **Tratamento de APIs** (requisi√ß√µes HTTP, JSON, tratamento de erros)

**Por que √© importante?** Python √© a linguagem mais usada em Data Engineering. Dominar os fundamentos √© essencial para construir pipelines robustos.

---

#### **2. Fundamentos de SQL para An√°lise de Dados**
- ‚úÖ **SELECT, FROM, WHERE** (consultas b√°sicas)
- ‚úÖ **ORDER BY e LIMIT** (ordena√ß√£o e limita√ß√£o de resultados)
- ‚úÖ **Alias (AS)** (renomea√ß√£o de colunas)
- ‚úÖ **Consultas anal√≠ticas** (√∫ltimo valor, m√°ximo, m√≠nimo, hist√≥rico)

**Por que √© importante?** SQL √© a linguagem padr√£o para consultar dados. Todo Data Engineer precisa saber SQL para criar dashboards, relat√≥rios e an√°lises.

---

#### **3. ETL (Extract, Transform, Load)**
- ‚úÖ **Extract (Extra√ß√£o)**: Consumir dados de APIs REST
- ‚úÖ **Transform (Transforma√ß√£o)**: Converter moedas, estruturar dados, adicionar metadados
- ‚úÖ **Load (Carga)**: Armazenar dados em formatos otimizados (Delta Lake)

**Por que √© importante?** ETL √© o cora√ß√£o da Engenharia de Dados. 90% do trabalho de um Data Engineer √© construir e manter pipelines ETL.

---

#### **4. PySpark e Processamento Distribu√≠do**
- ‚úÖ **DataFrames PySpark** (estrutura de dados distribu√≠da)
- ‚úÖ **Diferen√ßa entre Pandas e PySpark** (quando usar cada um)
- ‚úÖ **Processamento em escala** (prepara√ß√£o para Big Data)

**Por que √© importante?** PySpark permite processar terabytes de dados distribu√≠dos em clusters. √â essencial para projetos reais de Big Data.

---

#### **5. Databricks e Unity Catalog**
- ‚úÖ **Workspace Databricks** (ambiente de desenvolvimento)
- ‚úÖ **Unity Catalog** (governan√ßa de dados: Catalog ‚Üí Schema ‚Üí Table)
- ‚úÖ **Notebooks interativos** (desenvolvimento e documenta√ß√£o)

**Por que √© importante?** Databricks √© uma das plataformas mais usadas no mercado. Unity Catalog √© o padr√£o moderno para organiza√ß√£o de dados em Data Lakes.

---

#### **6. Delta Lake e Armazenamento ACID**
- ‚úÖ **Delta Tables** (tabelas com transa√ß√µes ACID)
- ‚úÖ **Time Travel** (acesso a vers√µes hist√≥ricas)
- ‚úÖ **Schema Evolution** (evolu√ß√£o autom√°tica do schema)
- ‚úÖ **Append incremental** (hist√≥rico de dados)

**Por que √© importante?** Delta Lake traz confiabilidade e performance para Data Lakes. √â o padr√£o moderno para armazenamento de dados anal√≠ticos.

---

#### **7. Automa√ß√£o com Databricks Workflows**
- ‚úÖ **Cria√ß√£o de workflows** (orquestra√ß√£o de pipelines)
- ‚úÖ **Par√¢metros Key-Value** (configura√ß√£o din√¢mica)
- ‚úÖ **Execu√ß√£o agendada** (automa√ß√£o de coleta de dados)

**Por que √© importante?** Pipelines precisam rodar automaticamente. Workflows permitem orquestrar e agendar execu√ß√µes sem interven√ß√£o manual.

---

#### **8. Visualiza√ß√£o e Dashboards**
- ‚úÖ **Queries SQL para dashboards** (4 queries principais)
- ‚úÖ **Visualiza√ß√£o de dados** (gr√°ficos, m√©tricas, hist√≥rico)
- ‚úÖ **An√°lise de tend√™ncias** (evolu√ß√£o temporal de pre√ßos)

**Por que √© importante?** Dados sem visualiza√ß√£o n√£o geram valor. Dashboards permitem que stakeholders tomem decis√µes baseadas em dados.

---

### **Resultado Final da Aula**

Ao final do workshop, voc√™ ter√° constru√≠do:

1. ‚úÖ **Pipeline ETL completo** funcionando em produ√ß√£o
2. ‚úÖ **Conhecimento pr√°tico** de Python, SQL, PySpark e Databricks
3. ‚úÖ **Portf√≥lio real** para mostrar em entrevistas
4. ‚úÖ **Base s√≥lida** para avan√ßar em Data Engineering

### **Pr√©-requisitos**

- ‚úÖ Conhecimento b√°sico de programa√ß√£o (qualquer linguagem)
- ‚úÖ Acesso √† internet
- ‚úÖ Conta gratuita no Databricks (criaremos durante a aula)
- ‚úÖ Vontade de aprender! üöÄ

---

## üìä **Esquema do Projeto**

Visualize a arquitetura completa do projeto: [app.excalidraw.com](https://app.excalidraw.com/s/8pvW6zbNUnD/9zZctm3OR9f)

---

## üéØ **Overview do Projeto**

### **Objetivo Principal**  

Desenvolver um pipeline ETL automatizado para consumir dados da **API da Coinbase** (pre√ßo do Bitcoin em USD) e da **API CurrencyFreaks** (cota√ß√£o USD-BRL), transformar os dados convertendo o valor para Real Brasileiro, e armazenar em **Delta Tables** no Databricks usando **Unity Catalog**.

---

## üì¶ **Requisitos do Projeto**

### **Requisitos Funcionais**

1. **Extra√ß√£o de Dados**
   - Extrair pre√ßo atual do Bitcoin em USD da API Coinbase
   - Extrair cota√ß√£o USD-BRL da API CurrencyFreaks
   - Tratamento de erros e retry logic

2. **Transforma√ß√£o de Dados**
   - Converter valor de USD para BRL usando cota√ß√£o em tempo real
   - Adicionar timestamp de processamento
   - Estruturar dados em formato tabular

3. **Carga de Dados**
   - Armazenar dados em Delta Table no Unity Catalog
   - Suportar append incremental (hist√≥rico de pre√ßos)
   - Garantir schema evolution autom√°tico

4. **Automa√ß√£o**
   - Pipeline execut√°vel via Databricks Workflows
   - Configura√ß√£o de par√¢metros via Key-Value pairs
   - Execu√ß√£o agendada ou manual

### **Requisitos T√©cnicos**

- **Python 3.8+**
- **Databricks Workspace** (conta gratuita dispon√≠vel)
- **APIs Externas:**
  - Coinbase API (p√∫blica, sem autentica√ß√£o)
  - CurrencyFreaks API (requer API key)
- **Bibliotecas Python:**
  - `requests` - Para requisi√ß√µes HTTP
  - `pyspark` - Para processamento distribu√≠do (dispon√≠vel no Databricks)
  - `datetime` - Para manipula√ß√£o de timestamps

### **Requisitos de Infraestrutura**

- **Databricks Workspace** configurado
- **Unity Catalog** habilitado
- **Cluster Databricks** ou **SQL Warehouse** para execu√ß√£o
- **API Key CurrencyFreaks** configurada no pipeline

---

## üîÑ **Arquitetura ETL**

### **1. EXTRACT (Extra√ß√£o)**

#### **O que ser√° extra√≠do:**

**a) Dados do Bitcoin (API Coinbase)**
- **Fonte:** `https://api.coinbase.com/v2/prices/spot`
- **Dados extra√≠dos:**
  - `amount`: Pre√ßo atual do Bitcoin em USD
  - `base`: Criptomoeda (BTC)
  - `currency`: Moeda de refer√™ncia (USD)
- **Formato:** JSON
- **Frequ√™ncia:** Tempo real (on-demand)

**b) Cota√ß√£o USD-BRL (API CurrencyFreaks)**
- **Fonte:** `https://api.currencyfreaks.com/v2.0/rates/latest`
- **Dados extra√≠dos:**
  - `rates.BRL`: Taxa de convers√£o USD para BRL
  - `date`: Data/hora da cota√ß√£o
  - `base`: Moeda base (USD)
- **Formato:** JSON
- **Autentica√ß√£o:** API Key (via par√¢metros do pipeline)
- **Frequ√™ncia:** Tempo real (on-demand)

#### **Fun√ß√µes de Extra√ß√£o:**

```python
def extrair_dados_bitcoin():
    """Extrai o JSON completo da API da Coinbase."""
    url = 'https://api.coinbase.com/v2/prices/spot'
    resposta = requests.get(url)
    return resposta.json()

def extrair_cotacao_usd_brl():
    """Extrai a cota√ß√£o USD-BRL da API CurrencyFreaks."""
    api_key = dbutils.widgets.get("api_key")
    url = f'https://api.currencyfreaks.com/v2.0/rates/latest?apikey={api_key}'
    resposta = requests.get(url)
    return resposta.json()
```

---

### **2. TRANSFORM (Transforma√ß√£o)**

#### **O que ser√° transformado:**

1. **Convers√£o de Moeda**
   - Multiplicar valor USD pela taxa de convers√£o USD-BRL
   - Calcular valor equivalente em Real Brasileiro

2. **Estrutura√ß√£o de Dados**
   - Selecionar campos relevantes
   - Renomear colunas para portugu√™s
   - Adicionar timestamp de processamento

3. **Valida√ß√£o e Limpeza**
   - Converter tipos de dados (string ‚Üí float)
   - Garantir formato correto do timestamp (datetime object)

#### **Schema Final dos Dados:**

| Coluna | Tipo | Descri√ß√£o |
|--------|------|-----------|
| `valor_usd` | DOUBLE | Pre√ßo do Bitcoin em D√≥lar Americano |
| `valor_brl` | DOUBLE | Pre√ßo do Bitcoin em Real Brasileiro |
| `criptomoeda` | STRING | Nome da criptomoeda (BTC) |
| `moeda_original` | STRING | Moeda de refer√™ncia original (USD) |
| `taxa_conversao_usd_brl` | DOUBLE | Taxa de convers√£o utilizada |
| `timestamp` | TIMESTAMP | Data e hora do processamento |

#### **Fun√ß√£o de Transforma√ß√£o:**

```python
def tratar_dados_bitcoin(dados_json, taxa_usd_brl):
    """Transforma os dados brutos da API, renomeia colunas, 
    adiciona timestamp e converte para BRL."""
    valor_usd = float(dados_json['data']['amount'])
    criptomoeda = dados_json['data']['base']
    moeda_original = dados_json['data']['currency']
    
    # Convertendo de USD para BRL
    valor_brl = valor_usd * taxa_usd_brl
    
    # Adicionando timestamp como datetime object
    timestamp = datetime.now()
    
    dados_tratados = [{
        "valor_usd": valor_usd,
        "valor_brl": valor_brl,
        "criptomoeda": criptomoeda,
        "moeda_original": moeda_original,
        "taxa_conversao_usd_brl": taxa_usd_brl,
        "timestamp": timestamp
    }]
    
    return dados_tratados
```

---

### **3. LOAD (Carga)**

#### **Onde ser√° carregado:**

**Delta Table no Unity Catalog**

- **Catalog:** `pipeline_api_bitcoin`
- **Schema:** `bitcoin_data`
- **Tabela:** `bitcoin_data`
- **Caminho completo:** `pipeline_api_bitcoin.bitcoin_data.bitcoin_data`

#### **Caracter√≠sticas do Armazenamento:**

1. **Formato:** Delta Lake
   - ACID transactions
   - Time Travel (acesso a vers√µes hist√≥ricas)
   - Schema Evolution autom√°tico
   - Otimiza√ß√µes autom√°ticas

2. **Modo de Escrita:**
   - `append`: Adiciona novos registros sem sobrescrever
   - `mergeSchema`: Permite evolu√ß√£o do schema automaticamente

3. **Estrutura:**
   - Tabela criada automaticamente pelo `saveAsTable()`
   - Schema inferido do DataFrame PySpark
   - Timestamp como tipo TIMESTAMP (n√£o string)

#### **C√≥digo de Carga:**

```python
# Criar Spark DataFrame
df = spark.createDataFrame(dados_bitcoin_tratado)

# Caminho da tabela Delta no Unity Catalog
delta_table_path = "pipeline_api_bitcoin.bitcoin_data.bitcoin_data"

# Salvar como Delta Table (modo append)
df.write \
    .format("delta") \
    .mode("append") \
    .option("mergeSchema", "true") \
    .saveAsTable(delta_table_path)
```

---

## üîß **Stack Tecnol√≥gico**

### **1. Python**
- **Vers√£o:** 3.8+
- **Uso:** Linguagem principal do pipeline

### **2. Requests** (`requests`)
- **O que faz:** Biblioteca para fazer requisi√ß√µes HTTP
- **Por que usamos:** Para consumir as APIs da Coinbase e CurrencyFreaks
- **Vers√£o:** √öltima est√°vel

### **3. PySpark** (`pyspark`)
- **O que faz:** API Python para Apache Spark, framework de processamento distribu√≠do
- **Por que usamos:** 
  - Processar dados de forma distribu√≠da
  - Criar DataFrames estruturados
  - Salvar em Delta Tables
  - Preparado para escalar para grandes volumes
- **Disponibilidade:** J√° inclu√≠do no Databricks

### **4. Databricks**
- **O que faz:** Plataforma unificada de an√°lise de dados baseada em Spark
- **Por que usamos:**
  - Ambiente gerenciado para PySpark
  - Unity Catalog para governan√ßa de dados
  - Delta Lake nativo
  - Workflows para automa√ß√£o
  - Dashboards e visualiza√ß√µes
  - Agentes de IA integrados

### **5. Unity Catalog**
- **O que faz:** Sistema de governan√ßa de dados unificado no Databricks
- **Por que usamos:**
  - Organiza√ß√£o hier√°rquica (Catalog ‚Üí Schema ‚Üí Table)
  - Controle de acesso centralizado
  - Linhagem de dados
  - Auditoria e compliance

### **6. Delta Lake**
- **O que faz:** Camada de armazenamento open-source para Data Lakes
- **Por que usamos:**
  - Transa√ß√µes ACID
  - Time Travel (acesso a vers√µes hist√≥ricas)
  - Schema Evolution autom√°tico
  - Performance otimizada
  - Baseado em Parquet (columnar)

---

## üìö **Por Que Usar Databricks e PySpark?**

### **PySpark vs Pandas**

| Caracter√≠stica | Pandas | PySpark |
|----------------|--------|---------|
| **Escalabilidade** | Single machine (RAM limitada) | Distributed (clusters) |
| **Volume de Dados** | Megabytes a Gigabytes | Terabytes a Petabytes |
| **Processamento** | In-memory (RAM) | Distributed (disco + mem√≥ria) |
| **Uso** | An√°lise explorat√≥ria, datasets pequenos | Big Data, pipelines de produ√ß√£o |

**Neste projeto:** Usamos PySpark porque:
- ‚úÖ Preparado para escalar
- ‚úÖ Integra√ß√£o nativa com Databricks
- ‚úÖ Suporte a Delta Lake
- ‚úÖ Processamento distribu√≠do

### **Delta Lake vs Parquet Simples**

| Caracter√≠stica | Parquet | Delta Lake |
|----------------|---------|------------|
| **Transa√ß√µes ACID** | ‚ùå | ‚úÖ |
| **Time Travel** | ‚ùå | ‚úÖ |
| **Schema Evolution** | Manual | Autom√°tico |
| **Updates/Deletes** | ‚ùå | ‚úÖ |
| **Performance** | Boa | Otimizada |

**Neste projeto:** Usamos Delta Lake porque:
- ‚úÖ Hist√≥rico completo de pre√ßos (Time Travel)
- ‚úÖ Garantias de consist√™ncia (ACID)
- ‚úÖ Permite evoluir schema sem quebrar dados antigos
- ‚úÖ Otimiza√ß√µes autom√°ticas

---

## üöÄ **Como Usar**

### **Pr√©-requisitos**

- Conta no Databricks (gratuita para testes)
- API Key da CurrencyFreaks
- Cluster Databricks ou SQL Warehouse configurado
- Unity Catalog habilitado

### **Instala√ß√£o**

1. **Clone o reposit√≥rio:**
```bash
git clone https://github.com/seu-usuario/pipeline-api-bitcoin-com-databricks.git
cd pipeline-api-bitcoin-com-databricks
```

2. **Importe os notebooks no Databricks:**
   - Acesse seu workspace Databricks
   - V√° em **Workspace** ‚Üí **Import**
   - Selecione os arquivos `.py` da pasta `src/`

### **Configura√ß√£o do Pipeline**

1. **Criar Workflow no Databricks:**
   - Acesse **Workflows** ‚Üí **Create**
   - Adicione uma task do tipo **Notebook**
   - Selecione o notebook `get_bitcoin_macro.py`

2. **Configurar Par√¢metros (Key-Value):**
   - Na se√ß√£o **Parameters** da task
   - Adicione:
     - **Key:** `api_key`
     - **Value:** `sua_api_key_aqui`

3. **Configurar Cluster:**
   - Selecione um cluster existente ou crie um novo
   - Recomendado: Single Node ou Small cluster para testes

4. **Executar Pipeline:**
   - Clique em **Run now** para execu√ß√£o manual
   - Ou configure um **Schedule** para execu√ß√£o autom√°tica

### **Verificar Dados**

Ap√≥s a execu√ß√£o, os dados estar√£o dispon√≠veis em:

```sql
SELECT * FROM pipeline_api_bitcoin.bitcoin_data.bitcoin_data
ORDER BY timestamp DESC
LIMIT 10;
```

---

## üìñ **Estrutura do Projeto**

```text
pipeline-api-bitcoin-com-databricks/
‚îú‚îÄ‚îÄ img/
‚îÇ   ‚îî‚îÄ‚îÄ SeUZPWKQ.png              # Imagem de capa do projeto
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ get_bitcoin_full.py       # Notebook completo com documenta√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ get_bitcoin_macro.py      # Script para workflows (simplificado)
‚îÇ   ‚îú‚îÄ‚îÄ aquecimento_python.py    # Tutorial de Python b√°sico
‚îÇ   ‚îî‚îÄ‚îÄ aquecimento_sql.py       # Tutorial de SQL b√°sico
‚îú‚îÄ‚îÄ README.md                     # Este arquivo
‚îî‚îÄ‚îÄ .gitignore                    # Arquivos ignorados pelo Git
```

### **Arquivos Principais**

- **`get_bitcoin_full.py`**: Notebook completo com explica√ß√µes detalhadas, exemplos de salvamento em JSON/CSV/Parquet/Delta, e visualiza√ß√µes
- **`get_bitcoin_macro.py`**: Vers√£o simplificada para uso em workflows, apenas salva em Delta Table
- **`aquecimento_python.py`**: Tutorial de Python b√°sico (vari√°veis, print, m√©todos)
- **`aquecimento_sql.py`**: Tutorial de SQL b√°sico (SELECT, WHERE, ORDER BY, LIMIT)

---

## üîÑ **Fluxo Completo do Pipeline**

```mermaid
flowchart TB
    A["üåê API Coinbase<br/><b>Bitcoin USD</b>"] --> E["üì• EXTRACT"]
    B["üåê API CurrencyFreaks<br/><b>USD-BRL Rate</b>"] --> E
    E --> T["üîÑ TRANSFORM<br/>‚Ä¢ Convert USD‚ÜíBRL<br/>‚Ä¢ Add timestamp<br/>‚Ä¢ Structure data"]
    T --> L["üíæ LOAD<br/>Delta Table<br/>Unity Catalog"]
    L --> W["‚öôÔ∏è WORKFLOW<br/>Databricks Jobs<br/>Automa√ß√£o"]
    W --> D["üìä DASHBOARD<br/>Visualiza√ß√µes<br/>M√©tricas em Tempo Real"]
    
    style A fill:#e1f5ff,stroke:#0066cc,stroke-width:2px
    style B fill:#e1f5ff,stroke:#0066cc,stroke-width:2px
    style E fill:#fff4e1,stroke:#ff9900,stroke-width:2px
    style T fill:#ffe1f5,stroke:#cc0066,stroke-width:2px
    style L fill:#e1ffe1,stroke:#00cc66,stroke-width:2px
    style W fill:#f0e1ff,stroke:#9900cc,stroke-width:2px
    style D fill:#ffe1f0,stroke:#cc0099,stroke-width:2px
```

---

## üìä **Queries do Dashboard**

O dashboard utiliza **4 queries SQL** para exibir diferentes visualiza√ß√µes dos dados do Bitcoin. Todas as queries consultam a tabela Delta `pipeline_api_bitcoin.bitcoin_data.bitcoin_data`.

### **1. LAST PRICE ‚Äî √öltimo Pre√ßo Registrado**

Retorna o pre√ßo mais recente do Bitcoin e o timestamp da coleta.

```sql
SELECT
  valor_brl        AS last_price,
  timestamp        AS last_timestamp
FROM pipeline_api_bitcoin.bitcoin_data.bitcoin_data
WHERE criptomoeda = 'BTC'
ORDER BY timestamp DESC
LIMIT 1
```

**Uso no Dashboard:** Exibido como valor principal em BRL (card ou m√©trica).

---

### **2. MAX PRICE ‚Äî Maior Pre√ßo Hist√≥rico**

Retorna o maior pre√ßo j√° registrado e quando ocorreu.

```sql
SELECT
  valor_brl  AS max_price,
  timestamp  AS max_timestamp
FROM pipeline_api_bitcoin.bitcoin_data.bitcoin_data
WHERE criptomoeda = 'BTC'
ORDER BY valor_brl DESC, timestamp DESC
LIMIT 1
```

**Uso no Dashboard:** Exibido como m√©trica de m√°ximo hist√≥rico em BRL.

---

### **3. MIN PRICE ‚Äî Menor Pre√ßo Hist√≥rico**

Retorna o menor pre√ßo j√° registrado e quando ocorreu.

```sql
SELECT
  valor_brl  AS min_price,
  timestamp  AS min_timestamp
FROM pipeline_api_bitcoin.bitcoin_data.bitcoin_data
WHERE criptomoeda = 'BTC'
ORDER BY valor_brl ASC, timestamp DESC
LIMIT 1
```

**Uso no Dashboard:** Exibido como m√©trica de m√≠nimo hist√≥rico em BRL.

---

### **4. HIST√ìRICO DE PRE√áOS ‚Äî Gr√°fico de Linha**

Retorna o hist√≥rico completo de pre√ßos ordenado por timestamp para visualiza√ß√£o em gr√°fico.

```sql
SELECT
  timestamp,
  valor_usd,
  valor_brl,
  taxa_conversao_usd_brl
FROM pipeline_api_bitcoin.bitcoin_data.bitcoin_data
WHERE criptomoeda = 'BTC'
ORDER BY timestamp ASC
```

**Uso no Dashboard:** Exibido como gr√°fico de linha mostrando a evolu√ß√£o do pre√ßo ao longo do tempo.

**Varia√ß√µes poss√≠veis:**
- √öltimas N horas: `WHERE timestamp >= CURRENT_TIMESTAMP - INTERVAL '24' HOUR`
- √öltimos N registros: `ORDER BY timestamp DESC LIMIT 100`
- Agrega√ß√£o por hora: `GROUP BY DATE_TRUNC('hour', timestamp)`

---

### **üìã Resumo das Queries**

| Query | Ordena√ß√£o | Limite | Uso |
|-------|-----------|--------|-----|
| **LAST PRICE** | `ORDER BY timestamp DESC` | `LIMIT 1` | Card/M√©trica principal (BRL) |
| **MAX PRICE** | `ORDER BY valor_brl DESC` | `LIMIT 1` | M√©trica de m√°ximo (BRL) |
| **MIN PRICE** | `ORDER BY valor_brl ASC` | `LIMIT 1` | M√©trica de m√≠nimo (BRL) |
| **HIST√ìRICO** | `ORDER BY timestamp ASC` | Sem limite | Gr√°fico de linha |

**Todas as queries usam:**
- `WHERE criptomoeda = 'BTC'` (filtro)
- `AS` (alias para facilitar uso no dashboard)

---

## üìä **Exemplo de Dados Gerados**

### **Estrutura da Tabela Delta:**

| valor_usd | valor_brl | criptomoeda | moeda_original | taxa_conversao_usd_brl | timestamp |
|-----------|-----------|-------------|---------------|------------------------|-----------|
| 43250.50 | 236125.48 | BTC | USD | 5.4567 | 2025-01-16 14:30:00 |
| 43280.25 | 236245.12 | BTC | USD | 5.4572 | 2025-01-16 14:31:00 |
| 43295.00 | 236310.15 | BTC | USD | 5.4575 | 2025-01-16 14:32:00 |

---

## üéì **Workshop**

Este projeto foi desenvolvido durante um workshop ao vivo. Assista a grava√ß√£o completa:

üîó [YouTube Live - Workshop Data Engineering](https://www.youtube.com/live/pFJCL1S3Zj8)

**Data:** 16/12/2025 √†s 19h30

---

## üîê **Seguran√ßa e Boas Pr√°ticas**

### **API Keys**

- ‚úÖ **Nunca** commite API keys no c√≥digo
- ‚úÖ Use par√¢metros do pipeline (Key-Value) ou Databricks Secrets
- ‚úÖ Configure permiss√µes adequadas no Unity Catalog

### **Governan√ßa de Dados**

- ‚úÖ Use Unity Catalog para organiza√ß√£o hier√°rquica
- ‚úÖ Defina schemas claros e descritivos
- ‚úÖ Documente tabelas e colunas
- ‚úÖ Configure pol√≠ticas de reten√ß√£o de dados

---

## ‚öôÔ∏è **Jobs & Pipelines - Orquestra√ß√£o do Pipeline**

Para automatizar a execu√ß√£o do nosso pipeline ETL, vamos criar um **Job** no Databricks que orquestra a execu√ß√£o do script `get_bitcoin_macro.py`.

### **O que s√£o Jobs & Pipelines?**

**Jobs** s√£o tarefas agendadas ou acionadas manualmente no Databricks que executam notebooks ou scripts Python. Eles permitem:

- ‚úÖ **Automa√ß√£o**: Executar o pipeline em intervalos regulares (a cada hora, dia, etc.)
- ‚úÖ **Orquestra√ß√£o**: Coordenar m√∫ltiplas tarefas em sequ√™ncia
- ‚úÖ **Monitoramento**: Acompanhar execu√ß√µes, logs e hist√≥rico
- ‚úÖ **Par√¢metros**: Passar configura√ß√µes din√¢micas (como API keys) via Key-Value pairs
- ‚úÖ **Alertas**: Notifica√ß√µes em caso de falha ou sucesso

### **Configura√ß√£o do Pipeline**

O pipeline √© configurado com:

1. **Task Principal**: Executa o notebook `get_bitcoin_macro.py`
2. **Par√¢metros Key-Value**: 
   - `api_key`: API key da CurrencyFreaks para convers√£o de moedas
3. **Cluster**: Cluster Databricks para processamento
4. **Schedule** (opcional): Agendamento autom√°tico para coleta peri√≥dica

### **Visualiza√ß√£o do Pipeline**

![Pipeline Configuration](./img/pipeline.png)

**Estrutura do Pipeline:**
- üìù **Task**: `get_bitcoin_macro` - Executa o script de extra√ß√£o e carga
- üîë **Parameters**: Configura√ß√£o da API key via Key-Value
- ‚öôÔ∏è **Cluster**: Ambiente de execu√ß√£o
- üìä **Output**: Dados salvos na Delta Table `pipeline_api_bitcoin.bitcoin_data.bitcoin_data`

---

## üìä **Dashboard - Visualiza√ß√£o dos Dados**

Ap√≥s coletar os dados, vamos criar um **Dashboard interativo** no Databricks para visualizar e monitorar os pre√ßos do Bitcoin em tempo real.

### **O que √© um Dashboard?**

Um **Dashboard** √© uma interface visual que exibe m√©tricas, gr√°ficos e an√°lises dos dados coletados. Ele permite:

- ‚úÖ **Visualiza√ß√£o em Tempo Real**: Ver os √∫ltimos pre√ßos coletados
- ‚úÖ **M√©tricas Principais**: √öltimo pre√ßo, m√°ximo hist√≥rico, m√≠nimo hist√≥rico
- ‚úÖ **Gr√°ficos Temporais**: Evolu√ß√£o do pre√ßo ao longo do tempo
- ‚úÖ **An√°lise R√°pida**: Identificar tend√™ncias e padr√µes
- ‚úÖ **Compartilhamento**: Compartilhar insights com stakeholders

### **Componentes do Dashboard**

O dashboard utiliza **4 queries SQL** para exibir diferentes visualiza√ß√µes:

1. **LAST PRICE**: √öltimo pre√ßo em BRL coletado
2. **MAX PRICE**: Maior pre√ßo hist√≥rico em BRL
3. **MIN PRICE**: Menor pre√ßo hist√≥rico em BRL
4. **HIST√ìRICO**: Gr√°fico de linha com evolu√ß√£o temporal

### **Visualiza√ß√£o do Dashboard**

![Dashboard Bitcoin](./img/dashboard.png)

**Elementos do Dashboard:**
- üìà **Cards de M√©tricas**: √öltimo pre√ßo, m√°ximo e m√≠nimo
- üìä **Gr√°fico de Linha**: Evolu√ß√£o do pre√ßo ao longo do tempo
- üîÑ **Atualiza√ß√£o Autom√°tica**: Dados atualizados conforme o pipeline executa
- üí∞ **Valores em BRL**: Todos os valores exibidos em Real Brasileiro

### **Como Criar o Dashboard**

1. Acesse **SQL** ‚Üí **Dashboards** no Databricks
2. Crie um novo dashboard
3. Adicione visualiza√ß√µes usando as queries SQL documentadas
4. Configure atualiza√ß√£o autom√°tica
5. Compartilhe com sua equipe

---

## üìù **Licen√ßa**

Este projeto √© parte do conte√∫do educacional da **Jornada de Dados**.

---

## üë• **Contato**

- **Website**: [suajornadadedados.com.br](https://suajornadadedados.com.br/)
- **YouTube**: [Canal Jornada de Dados](https://www.youtube.com/@JornadadeDados)

---

<p align="center">
  <em>Desenvolvido com ‚ù§Ô∏è pela equipe Jornada de Dados</em>
</p>
