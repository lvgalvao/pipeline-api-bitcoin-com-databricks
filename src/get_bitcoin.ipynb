{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28495a62-c29d-42f9-98bb-3ef318bb44d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # üí∞ Extra√ß√£o de Dados Bitcoin - API Coinbase\n",
    "# MAGIC\n",
    "# MAGIC Este notebook demonstra como:\n",
    "# MAGIC - Extrair dados da API da Coinbase\n",
    "# MAGIC - Adicionar timestamp aos dados\n",
    "# MAGIC - Atualizar e salvar dados em JSON\n",
    "# MAGIC - Salvar dados em formato **Parquet** (otimizado para Big Data)\n",
    "# MAGIC - Salvar dados em formato **CSV** (leg√≠vel por humanos)\n",
    "# MAGIC - Trabalhar com **Volumes** no Databricks\n",
    "# MAGIC\n",
    "# MAGIC ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c723000e-a6c1-4810-8ea6-f93c1da96c63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 1. Importando Bibliotecas Necess√°rias\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Para trabalhar com volumes no Databricks\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "\n",
    "print(\"‚úÖ Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36c7d607-e7fe-4801-a8c7-f5affa6439af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 2. Configurando URLs e Par√¢metros da API\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# URL da API Coinbase para obter o pre√ßo spot\n",
    "print(\"üìå Configura√ß√£o da API Coinbase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 3. Extraindo Dados da API Coinbase\n",
    "\n",
    "# COMMAND ----------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_dados_bitcoin():\n",
    "    \"\"\"Extrai o JSON completo da API da Coinbase.\"\"\"\n",
    "    url = 'https://api.coinbase.com/v2/prices/spot'\n",
    "    resposta = requests.get(url)\n",
    "    return resposta.json()\n",
    "\n",
    "# Extraindo dados\n",
    "dados_json = extrair_dados_bitcoin()\n",
    "print(\"‚úÖ Dados extra√≠dos com sucesso!\")\n",
    "print(f\"\\nResposta da API:\")\n",
    "print(json.dumps(dados_json, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 4. Tratando Dados e Adicionando Timestamp\n",
    "\n",
    "# COMMAND ----------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tratar_dados_bitcoin(dados_json):\n",
    "    \"\"\"Transforma os dados brutos da API, renomeia colunas e adiciona timestamp.\"\"\"\n",
    "    valor = dados_json['data']['amount']\n",
    "    criptomoeda = dados_json['data']['base']\n",
    "    moeda = dados_json['data']['currency']\n",
    "    \n",
    "    # Adicionando timestamp (importante para rastrear quando o dado foi coletado)\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    timestamp_readable = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    dados_tratados = [{\n",
    "        \"valor\": valor,\n",
    "        \"criptomoeda\": criptomoeda,\n",
    "        \"moeda\": moeda,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"timestamp_readable\": timestamp_readable\n",
    "    }]\n",
    "    \n",
    "    return dados_tratados\n",
    "\n",
    "# Tratando os dados\n",
    "dados_bitcoin = tratar_dados_bitcoin(dados_json)\n",
    "print(\"‚úÖ Dados tratados com sucesso!\")\n",
    "print(f\"\\nDados processados:\")\n",
    "print(json.dumps(dados_bitcoin, indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 5. Configurando Volume no Databricks\n",
    "\n",
    "# MAGIC ### üìÅ O que s√£o Volumes no Databricks?\n",
    "# MAGIC\n",
    "# MAGIC **Volumes** s√£o locais de armazenamento gerenciados pelo Databricks que permitem:\n",
    "# MAGIC - ‚úÖ **Organiza√ß√£o**: Estruturar dados de forma hier√°rquica\n",
    "# MAGIC - ‚úÖ **Acesso F√°cil**: Montados diretamente no sistema de arquivos\n",
    "# MAGIC - ‚úÖ **Colabora√ß√£o**: Compartilhados entre usu√°rios e notebooks\n",
    "# MAGIC - ‚úÖ **Performance**: Otimizados para leitura/escrita\n",
    "# MAGIC\n",
    "# MAGIC **Formato do caminho**: `/Volumes/<catalog>/<schema>/<volume>/<path>`\n",
    "# MAGIC\n",
    "# MAGIC No nosso caso, vamos usar: `/Volumes/main/default/bitcoin_data`\n",
    "\n",
    "# COMMAND ----------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo caminho do volume no Databricks\n",
    "# Formato: /Volumes/<catalog>/<schema>/<volume>/<path>\n",
    "volume_path = \"/Volumes/main/default/bitcoin_data\"\n",
    "\n",
    "# Criando estrutura de diret√≥rios\n",
    "directories = {\n",
    "    \"json\": f\"{volume_path}/json\",\n",
    "    \"parquet\": f\"{volume_path}/parquet\",\n",
    "    \"csv\": f\"{volume_path}/csv\"\n",
    "}\n",
    "\n",
    "# Criando diret√≥rios se n√£o existirem\n",
    "for dir_type, dir_path in directories.items():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    print(f\"‚úÖ Diret√≥rio {dir_type}: {dir_path}\")\n",
    "\n",
    "print(f\"\\nüìÅ Volume configurado: {volume_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 6. Salvando e Atualizando JSON\n",
    "\n",
    "# MAGIC JSON √© um formato de texto leg√≠vel por humanos, ideal para armazenar dados estruturados de forma simples.\n",
    "\n",
    "# COMMAND ----------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho do arquivo JSON no volume do Databricks\n",
    "json_file = f\"{directories['json']}/bitcoin_data.json\"\n",
    "\n",
    "# Carregar dados existentes (se houver) - isso permite atualiza√ß√£o incremental\n",
    "dados_existentes = []\n",
    "if os.path.exists(json_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        dados_existentes = json.load(f)\n",
    "    print(f\"üìÇ Arquivo JSON existente encontrado com {len(dados_existentes)} registros\")\n",
    "\n",
    "# Adicionar novo registro (dados_bitcoin √© uma lista)\n",
    "dados_existentes.extend(dados_bitcoin)\n",
    "\n",
    "# Salvar JSON atualizado no volume\n",
    "with open(json_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(dados_existentes, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ JSON atualizado e salvo no volume: {json_file}\")\n",
    "print(f\"üìä Total de registros: {len(dados_existentes)}\")\n",
    "\n",
    "# Mostrar √∫ltimos 3 registros\n",
    "print(f\"\\n√öltimos 3 registros:\")\n",
    "for registro in dados_existentes[-3:]:\n",
    "    valor_formatado = float(registro['valor'])\n",
    "    print(f\"  - {registro['timestamp_readable']}: ${valor_formatado:,.2f} {registro['moeda']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 7. Convertendo para DataFrame e Salvando como Parquet\n",
    "\n",
    "# MAGIC ### üìä O que √© Parquet?\n",
    "# MAGIC\n",
    "# MAGIC **Parquet** √© um formato de arquivo **bin√°rio e columnar** otimizado para Big Data:\n",
    "# MAGIC\n",
    "# MAGIC **Caracter√≠sticas:**\n",
    "# MAGIC - ‚úÖ **Compress√£o**: Arquivos muito menores que CSV (at√© 90% de economia)\n",
    "# MAGIC - ‚úÖ **Performance**: Leitura r√°pida, especialmente para consultas anal√≠ticas\n",
    "# MAGIC - ‚úÖ **Big Data**: Ideal para processar grandes volumes de dados (terabytes/petabytes)\n",
    "# MAGIC - ‚úÖ **Schema**: Mant√©m informa√ß√µes sobre tipos de dados automaticamente\n",
    "# MAGIC - ‚úÖ **Columnar**: Armazena dados por coluna, n√£o por linha (otimizado para analytics)\n",
    "# MAGIC - ‚ùå **N√£o √© leg√≠vel por humanos**: Precisa de ferramentas especiais para ler (Pandas, Spark, etc.)\n",
    "# MAGIC\n",
    "# MAGIC **Quando usar Parquet?**\n",
    "# MAGIC - Processamento de grandes volumes de dados\n",
    "# MAGIC - Data Lakes e Data Warehouses\n",
    "# MAGIC - Quando performance e economia de espa√ßo s√£o importantes\n",
    "# MAGIC - An√°lises anal√≠ticas (OLAP)\n",
    "\n",
    "# COMMAND ----------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar DataFrame do Pandas a partir dos dados tratados\n",
    "df = pd.DataFrame(dados_bitcoin)\n",
    "\n",
    "# Mostrar DataFrame\n",
    "print(\"üìä DataFrame criado:\")\n",
    "print(df)\n",
    "print(f\"\\nTipos de dados:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Caminho do arquivo Parquet no volume do Databricks\n",
    "parquet_file = f\"{directories['parquet']}/bitcoin_data.parquet\"\n",
    "\n",
    "# Verificar se arquivo Parquet j√° existe para fazer append (atualiza√ß√£o incremental)\n",
    "if os.path.exists(parquet_file):\n",
    "    # Ler Parquet existente\n",
    "    df_existente = pd.read_parquet(parquet_file)\n",
    "    # Concatenar com novos dados\n",
    "    df_parquet_final = pd.concat([df_existente, df], ignore_index=True)\n",
    "    print(f\"üìÇ Parquet existente encontrado com {len(df_existente)} registros\")\n",
    "else:\n",
    "    df_parquet_final = df\n",
    "    print(\"üìù Criando novo arquivo Parquet\")\n",
    "\n",
    "# Salvar como Parquet no volume\n",
    "df_parquet_final.to_parquet(parquet_file, index=False, engine='pyarrow')\n",
    "\n",
    "# Verificar tamanho do arquivo\n",
    "tamanho_parquet = os.path.getsize(parquet_file)\n",
    "print(f\"\\n‚úÖ Parquet salvo no volume: {parquet_file}\")\n",
    "print(f\"üì¶ Tamanho do arquivo: {tamanho_parquet:,} bytes ({tamanho_parquet/1024:.2f} KB)\")\n",
    "print(f\"üìä Total de registros no Parquet: {len(df_parquet_final)}\")\n",
    "\n",
    "# Ler e mostrar dados do Parquet para confirmar\n",
    "df_parquet_leitura = pd.read_parquet(parquet_file)\n",
    "print(f\"\\n‚úÖ √öltimas 3 linhas lidas do Parquet:\")\n",
    "print(df_parquet_leitura.tail(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 8. Salvando como CSV\n",
    "\n",
    "# MAGIC ### üìÑ O que √© CSV?\n",
    "# MAGIC\n",
    "# MAGIC **CSV (Comma-Separated Values)** √© um formato de arquivo **texto** simples e universal:\n",
    "# MAGIC\n",
    "# MAGIC **Caracter√≠sticas:**\n",
    "# MAGIC - ‚úÖ **Leg√≠vel por humanos**: Pode ser aberto no Excel, Bloco de Notas, Google Sheets, etc.\n",
    "# MAGIC - ‚úÖ **Simples**: F√°cil de entender e debugar\n",
    "# MAGIC - ‚úÖ **Universal**: Suportado por praticamente todas as ferramentas\n",
    "# MAGIC - ‚úÖ **Row-based**: Armazena dados por linha (otimizado para transa√ß√µes)\n",
    "# MAGIC - ‚ùå **Maior tamanho**: Arquivos maiores que Parquet (sem compress√£o)\n",
    "# MAGIC - ‚ùå **Mais lento**: Leitura e escrita mais lentas em grandes volumes\n",
    "# MAGIC - ‚ùå **Sem schema**: N√£o preserva tipos de dados automaticamente (tudo √© texto)\n",
    "# MAGIC\n",
    "# MAGIC **Quando usar CSV?**\n",
    "# MAGIC - Dados pequenos ou m√©dios\n",
    "# MAGIC - Quando precisa ser leg√≠vel por humanos\n",
    "# MAGIC - Integra√ß√£o com ferramentas que n√£o suportam Parquet\n",
    "# MAGIC - Debugging e inspe√ß√£o manual dos dados\n",
    "# MAGIC - Exporta√ß√£o para Excel ou outras ferramentas de neg√≥cio\n",
    "\n",
    "# COMMAND ----------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho do arquivo CSV no volume do Databricks\n",
    "csv_file = f\"{directories['csv']}/bitcoin_data.csv\"\n",
    "\n",
    "# Criar DataFrame a partir dos dados tratados\n",
    "df_csv_novo = pd.DataFrame(dados_bitcoin)\n",
    "\n",
    "# Verificar se arquivo CSV j√° existe (atualiza√ß√£o incremental)\n",
    "if os.path.exists(csv_file):\n",
    "    # Ler CSV existente e adicionar nova linha\n",
    "    df_csv_existente = pd.read_csv(csv_file)\n",
    "    df_csv_atualizado = pd.concat([df_csv_existente, df_csv_novo], ignore_index=True)\n",
    "    print(f\"üìÇ CSV existente encontrado com {len(df_csv_existente)} registros\")\n",
    "else:\n",
    "    # Criar novo CSV\n",
    "    df_csv_atualizado = df_csv_novo\n",
    "    print(\"üìù Criando novo arquivo CSV\")\n",
    "\n",
    "# Salvar CSV no volume (append mode ou criar novo)\n",
    "df_csv_atualizado.to_csv(csv_file, index=False, encoding='utf-8')\n",
    "\n",
    "# Verificar tamanho do arquivo\n",
    "tamanho_csv = os.path.getsize(csv_file)\n",
    "\n",
    "print(f\"\\n‚úÖ CSV salvo no volume: {csv_file}\")\n",
    "print(f\"üì¶ Tamanho do arquivo: {tamanho_csv:,} bytes ({tamanho_csv/1024:.2f} KB)\")\n",
    "print(f\"üìä Total de registros no CSV: {len(df_csv_atualizado)}\")\n",
    "\n",
    "# Mostrar √∫ltimas linhas\n",
    "print(f\"\\n‚úÖ √öltimas 3 linhas do CSV:\")\n",
    "print(df_csv_atualizado.tail(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 9. Compara√ß√£o Detalhada: CSV vs Parquet\n",
    "\n",
    "# MAGIC Vamos comparar os tamanhos dos arquivos e entender as diferen√ßas pr√°ticas entre os dois formatos:\n",
    "\n",
    "# COMMAND ----------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparando tamanhos dos arquivos salvos no volume\n",
    "parquet_file = f\"{directories['parquet']}/bitcoin_data.parquet\"\n",
    "csv_file = f\"{directories['csv']}/bitcoin_data.csv\"\n",
    "\n",
    "if os.path.exists(parquet_file) and os.path.exists(csv_file):\n",
    "    tamanho_parquet = os.path.getsize(parquet_file)\n",
    "    tamanho_csv = os.path.getsize(csv_file)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"COMPARA√á√ÉO: CSV vs PARQUET\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nüìÑ CSV:\")\n",
    "    print(f\"   Tamanho: {tamanho_csv:,} bytes ({tamanho_csv/1024:.2f} KB)\")\n",
    "    \n",
    "    print(f\"\\nüìä Parquet:\")\n",
    "    print(f\"   Tamanho: {tamanho_parquet:,} bytes ({tamanho_parquet/1024:.2f} KB)\")\n",
    "    \n",
    "    if tamanho_csv > 0:\n",
    "        economia = ((tamanho_csv - tamanho_parquet) / tamanho_csv) * 100\n",
    "        print(f\"\\nüíæ Economia com Parquet: {economia:.1f}%\")\n",
    "        print(f\"   (CSV √© {tamanho_csv/tamanho_parquet:.1f}x maior que Parquet)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DIFEREN√áAS PR√ÅTICAS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\nüìÑ CSV (Comma-Separated Values):\")\n",
    "    print(\"  ‚úÖ Leg√≠vel por humanos (Excel, Bloco de Notas)\")\n",
    "    print(\"  ‚úÖ F√°cil de debugar e inspecionar\")\n",
    "    print(\"  ‚úÖ Universal (suportado por todas as ferramentas)\")\n",
    "    print(\"  ‚úÖ Row-based (otimizado para transa√ß√µes)\")\n",
    "    print(\"  ‚ùå Arquivo maior (sem compress√£o)\")\n",
    "    print(\"  ‚ùå Leitura mais lenta em grandes volumes\")\n",
    "    print(\"  ‚ùå N√£o preserva tipos de dados (tudo √© texto)\")\n",
    "    \n",
    "    print(\"\\nüìä Parquet (Apache Parquet):\")\n",
    "    print(\"  ‚úÖ Arquivo menor (compress√£o eficiente)\")\n",
    "    print(\"  ‚úÖ Leitura muito mais r√°pida (columnar)\")\n",
    "    print(\"  ‚úÖ Mant√©m schema (tipos de dados)\")\n",
    "    print(\"  ‚úÖ Ideal para Big Data e Analytics\")\n",
    "    print(\"  ‚úÖ Columnar (otimizado para consultas anal√≠ticas)\")\n",
    "    print(\"  ‚ùå N√£o √© leg√≠vel diretamente por humanos\")\n",
    "    print(\"  ‚ùå Requer ferramentas especiais (Pandas, Spark)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"QUANDO USAR CADA UM?\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nüìÑ Use CSV quando:\")\n",
    "    print(\"  - Dados pequenos ou m√©dios (< 100MB)\")\n",
    "    print(\"  - Precisa ser leg√≠vel por humanos\")\n",
    "    print(\"  - Integra√ß√£o com Excel/ferramentas de neg√≥cio\")\n",
    "    print(\"  - Debugging e inspe√ß√£o manual\")\n",
    "    \n",
    "    print(\"\\nüìä Use Parquet quando:\")\n",
    "    print(\"  - Grandes volumes de dados (> 100MB)\")\n",
    "    print(\"  - Performance √© cr√≠tica\")\n",
    "    print(\"  - Data Lakes e Data Warehouses\")\n",
    "    print(\"  - An√°lises anal√≠ticas (OLAP)\")\n",
    "    print(\"  - Processamento com Spark/PySpark\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Arquivos n√£o encontrados. Execute as c√©lulas anteriores primeiro.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 10. Resumo do Pipeline\n",
    "\n",
    "# MAGIC Este pipeline completo realiza:\n",
    "# MAGIC\n",
    "# MAGIC 1. ‚úÖ **Extra√ß√£o**: Busca dados da API Coinbase\n",
    "# MAGIC 2. ‚úÖ **Transforma√ß√£o**: Trata e renomeia colunas, adiciona timestamp\n",
    "# MAGIC 3. ‚úÖ **Carga**: Salva em m√∫ltiplos formatos:\n",
    "# MAGIC    - JSON (atualiza√ß√£o incremental)\n",
    "# MAGIC    - Parquet (otimizado para Big Data)\n",
    "# MAGIC    - CSV (leg√≠vel por humanos)\n",
    "# MAGIC 4. ‚úÖ **Armazenamento**: Utiliza Volumes do Databricks para organiza√ß√£o\n",
    "# MAGIC\n",
    "# MAGIC **Pr√≥ximos passos**: Criar dashboard e agente de IA para an√°lise dos dados!\n",
    "\n",
    "# COMMAND ----------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "get_bitcoin",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
