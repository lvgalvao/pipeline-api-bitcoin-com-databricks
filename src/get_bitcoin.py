# Databricks notebook source
# MAGIC %md
# MAGIC # Extra√ß√£o de Dados Bitcoin - API Coinbase
# MAGIC
# MAGIC Este notebook demonstra como:
# MAGIC - Extrair dados da API da Coinbase
# MAGIC - Adicionar timestamp aos dados
# MAGIC - Atualizar e salvar dados em JSON
# MAGIC - Salvar dados em formato **Parquet** (otimizado para Big Data)
# MAGIC - Salvar dados em formato **CSV** (leg√≠vel por humanos)
# MAGIC - Trabalhar com **Volumes** no Databricks
# MAGIC
# MAGIC ---

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. Importando Bibliotecas Necess√°rias

# COMMAND ----------

import requests
# import json
# from datetime import datetime

print("‚úÖ Bibliotecas importadas com sucesso!")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. Configurando URLs e Par√¢metros da API

# COMMAND ----------

# URL da API Coinbase para obter o pre√ßo spot
url = 'https://api.coinbase.com/v2/prices/spot'

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Extraindo Dados da API Coinbase

# COMMAND ----------

resposta = requests.get(url)

# COMMAND ----------

print(resposta.json())

# COMMAND ----------

def extrair_dados_bitcoin():
    """Extrai o JSON completo da API da Coinbase."""
    url = 'https://api.coinbase.com/v2/prices/spot'
    resposta = requests.get(url)
    return resposta.json()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. Tratando Dados e Adicionando Timestamp

# COMMAND ----------

def tratar_dados_bitcoin(dados_json):
    """Transforma os dados brutos da API, renomeia colunas e adiciona timestamp."""
    valor = dados_json['data']['amount']
    criptomoeda = dados_json['data']['base']
    moeda = dados_json['data']['currency']
    
    # Adicionando timestamp (importante para rastrear quando o dado foi coletado)
    timestamp = datetime.now().isoformat()
    timestamp_readable = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    dados_tratados = [{
        "valor": valor,
        "criptomoeda": criptomoeda,
        "moeda": moeda,
        "timestamp": timestamp,
        "timestamp_readable": timestamp_readable
    }]
    
    return dados_tratados

# Tratando os dados
dados_bitcoin = tratar_dados_bitcoin(dados_json)
print("‚úÖ Dados tratados com sucesso!")
print(f"\nDados processados:")
print(json.dumps(dados_bitcoin, indent=2, ensure_ascii=False))

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5. Configurando noVolume no Databricks
# MAGIC
# MAGIC O que s√£o Volumes no Databricks?
# MAGIC
# MAGIC **Volumes** s√£o locais de armazenamento gerenciados pelo Databricks que permitem:
# MAGIC - ‚úÖ **Organiza√ß√£o**: Estruturar dados de forma hier√°rquica
# MAGIC - ‚úÖ **Acesso F√°cil**: Montados diretamente no sistema de arquivos
# MAGIC - ‚úÖ **Colabora√ß√£o**: Compartilhados entre usu√°rios e notebooks
# MAGIC - ‚úÖ **Performance**: Otimizados para leitura/escrita
# MAGIC
# MAGIC **Formato do caminho**: `/Volumes/<catalog>/<schema>/<volume>/<path>`
# MAGIC
# MAGIC No nosso caso, vamos usar: `/Volumes/main/default/bitcoin_data`

# COMMAND ----------

# Definindo caminho do volume no Databricks
# Formato: /Volumes/<catalog>/<schema>/<volume>/<path>
volume_path = "/Volumes/main/default/bitcoin_data"

# Criando estrutura de diret√≥rios
directories = {
    "json": f"{volume_path}/json",
    "parquet": f"{volume_path}/parquet",
    "csv": f"{volume_path}/csv"
}

# Criando diret√≥rios se n√£o existirem
for dir_type, dir_path in directories.items():
    os.makedirs(dir_path, exist_ok=True)
    print(f"‚úÖ Diret√≥rio {dir_type}: {dir_path}")

print(f"\nüìÅ Volume configurado: {volume_path}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 6. Salvando e Atualizando JSON
# MAGIC
# MAGIC JSON √© um formato de texto leg√≠vel por humanos, ideal para armazenar dados estruturados de forma simples.

# COMMAND ----------

# Caminho do arquivo JSON no volume do Databricks
json_file = f"{directories['json']}/bitcoin_data.json"

# Carregar dados existentes (se houver) - isso permite atualiza√ß√£o incremental
dados_existentes = []
if os.path.exists(json_file):
    with open(json_file, 'r', encoding='utf-8') as f:
        dados_existentes = json.load(f)
    print(f"üìÇ Arquivo JSON existente encontrado com {len(dados_existentes)} registros")

# Adicionar novo registro (dados_bitcoin √© uma lista)
dados_existentes.extend(dados_bitcoin)

# Salvar JSON atualizado no volume
with open(json_file, 'w', encoding='utf-8') as f:
    json.dump(dados_existentes, f, indent=2, ensure_ascii=False)

print(f"‚úÖ JSON atualizado e salvo no volume: {json_file}")
print(f"üìä Total de registros: {len(dados_existentes)}")

# Mostrar √∫ltimos 3 registros
print(f"\n√öltimos 3 registros:")
for registro in dados_existentes[-3:]:
    valor_formatado = float(registro['valor'])
    print(f"  - {registro['timestamp_readable']}: ${valor_formatado:,.2f} {registro['moeda']}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 7. Convertendo para DataFrame e Salvando como Parquet
# MAGIC
# MAGIC ### üìä O que √© Parquet?
# MAGIC
# MAGIC **Parquet** √© um formato de arquivo **bin√°rio e columnar** otimizado para Big Data:
# MAGIC
# MAGIC **Caracter√≠sticas:**
# MAGIC - ‚úÖ **Compress√£o**: Arquivos muito menores que CSV (at√© 90% de economia)
# MAGIC - ‚úÖ **Performance**: Leitura r√°pida, especialmente para consultas anal√≠ticas
# MAGIC - ‚úÖ **Big Data**: Ideal para processar grandes volumes de dados (terabytes/petabytes)
# MAGIC - ‚úÖ **Schema**: Mant√©m informa√ß√µes sobre tipos de dados automaticamente
# MAGIC - ‚úÖ **Columnar**: Armazena dados por coluna, n√£o por linha (otimizado para analytics)
# MAGIC - ‚ùå **N√£o √© leg√≠vel por humanos**: Precisa de ferramentas especiais para ler (Pandas, Spark, etc.)
# MAGIC
# MAGIC **Quando usar Parquet?**
# MAGIC - Processamento de grandes volumes de dados
# MAGIC - Data Lakes e Data Warehouses
# MAGIC - Quando performance e economia de espa√ßo s√£o importantes
# MAGIC - An√°lises anal√≠ticas (OLAP)

# COMMAND ----------

# Criar DataFrame do Pandas a partir dos dados tratados
df = pd.DataFrame(dados_bitcoin)

# Mostrar DataFrame
print("üìä DataFrame criado:")
print(df)
print(f"\nTipos de dados:")
print(df.dtypes)

# Caminho do arquivo Parquet no volume do Databricks
parquet_file = f"{directories['parquet']}/bitcoin_data.parquet"

# Verificar se arquivo Parquet j√° existe para fazer append (atualiza√ß√£o incremental)
if os.path.exists(parquet_file):
    # Ler Parquet existente
    df_existente = pd.read_parquet(parquet_file)
    # Concatenar com novos dados
    df_parquet_final = pd.concat([df_existente, df], ignore_index=True)
    print(f"üìÇ Parquet existente encontrado com {len(df_existente)} registros")
else:
    df_parquet_final = df
    print("üìù Criando novo arquivo Parquet")

# Salvar como Parquet no volume
df_parquet_final.to_parquet(parquet_file, index=False, engine='pyarrow')

# Verificar tamanho do arquivo
tamanho_parquet = os.path.getsize(parquet_file)
print(f"\n‚úÖ Parquet salvo no volume: {parquet_file}")
print(f"üì¶ Tamanho do arquivo: {tamanho_parquet:,} bytes ({tamanho_parquet/1024:.2f} KB)")
print(f"üìä Total de registros no Parquet: {len(df_parquet_final)}")

# Ler e mostrar dados do Parquet para confirmar
df_parquet_leitura = pd.read_parquet(parquet_file)
print(f"\n‚úÖ √öltimas 3 linhas lidas do Parquet:")
print(df_parquet_leitura.tail(3))

# COMMAND ----------

# MAGIC %md
# MAGIC ## 8. Salvando como CSV
# MAGIC
# MAGIC ### üìÑ O que √© CSV?
# MAGIC
# MAGIC **CSV (Comma-Separated Values)** √© um formato de arquivo **texto** simples e universal:
# MAGIC
# MAGIC **Caracter√≠sticas:**
# MAGIC - ‚úÖ **Leg√≠vel por humanos**: Pode ser aberto no Excel, Bloco de Notas, Google Sheets, etc.
# MAGIC - ‚úÖ **Simples**: F√°cil de entender e debugar
# MAGIC - ‚úÖ **Universal**: Suportado por praticamente todas as ferramentas
# MAGIC - ‚úÖ **Row-based**: Armazena dados por linha (otimizado para transa√ß√µes)
# MAGIC - ‚ùå **Maior tamanho**: Arquivos maiores que Parquet (sem compress√£o)
# MAGIC - ‚ùå **Mais lento**: Leitura e escrita mais lentas em grandes volumes
# MAGIC - ‚ùå **Sem schema**: N√£o preserva tipos de dados automaticamente (tudo √© texto)
# MAGIC
# MAGIC **Quando usar CSV?**
# MAGIC - Dados pequenos ou m√©dios
# MAGIC - Quando precisa ser leg√≠vel por humanos
# MAGIC - Integra√ß√£o com ferramentas que n√£o suportam Parquet
# MAGIC - Debugging e inspe√ß√£o manual dos dados
# MAGIC - Exporta√ß√£o para Excel ou outras ferramentas de neg√≥cio

# COMMAND ----------

# Caminho do arquivo CSV no volume do Databricks
csv_file = f"{directories['csv']}/bitcoin_data.csv"

# Criar DataFrame a partir dos dados tratados
df_csv_novo = pd.DataFrame(dados_bitcoin)

# Verificar se arquivo CSV j√° existe (atualiza√ß√£o incremental)
if os.path.exists(csv_file):
    # Ler CSV existente e adicionar nova linha
    df_csv_existente = pd.read_csv(csv_file)
    df_csv_atualizado = pd.concat([df_csv_existente, df_csv_novo], ignore_index=True)
    print(f"üìÇ CSV existente encontrado com {len(df_csv_existente)} registros")
else:
    # Criar novo CSV
    df_csv_atualizado = df_csv_novo
    print("üìù Criando novo arquivo CSV")

# Salvar CSV no volume (append mode ou criar novo)
df_csv_atualizado.to_csv(csv_file, index=False, encoding='utf-8')

# Verificar tamanho do arquivo
tamanho_csv = os.path.getsize(csv_file)

print(f"\n‚úÖ CSV salvo no volume: {csv_file}")
print(f"üì¶ Tamanho do arquivo: {tamanho_csv:,} bytes ({tamanho_csv/1024:.2f} KB)")
print(f"üìä Total de registros no CSV: {len(df_csv_atualizado)}")

# Mostrar √∫ltimas linhas
print(f"\n‚úÖ √öltimas 3 linhas do CSV:")
print(df_csv_atualizado.tail(3))

# COMMAND ----------

# MAGIC %md
# MAGIC ## 9. Compara√ß√£o Detalhada: CSV vs Parquet
# MAGIC
# MAGIC Vamos comparar os tamanhos dos arquivos e entender as diferen√ßas pr√°ticas entre os dois formatos:

# COMMAND ----------

# Comparando tamanhos dos arquivos salvos no volume
parquet_file = f"{directories['parquet']}/bitcoin_data.parquet"
csv_file = f"{directories['csv']}/bitcoin_data.csv"

if os.path.exists(parquet_file) and os.path.exists(csv_file):
    tamanho_parquet = os.path.getsize(parquet_file)
    tamanho_csv = os.path.getsize(csv_file)
    
    print("=" * 60)
    print("COMPARA√á√ÉO: CSV vs PARQUET")
    print("=" * 60)
    
    print(f"\nüìÑ CSV:")
    print(f"   Tamanho: {tamanho_csv:,} bytes ({tamanho_csv/1024:.2f} KB)")
    
    print(f"\nüìä Parquet:")
    print(f"   Tamanho: {tamanho_parquet:,} bytes ({tamanho_parquet/1024:.2f} KB)")
    
    if tamanho_csv > 0:
        economia = ((tamanho_csv - tamanho_parquet) / tamanho_csv) * 100
        print(f"\nüíæ Economia com Parquet: {economia:.1f}%")
        print(f"   (CSV √© {tamanho_csv/tamanho_parquet:.1f}x maior que Parquet)")
    
    print("\n" + "=" * 60)
    print("DIFEREN√áAS PR√ÅTICAS")
    print("=" * 60)
    
    print("\nüìÑ CSV (Comma-Separated Values):")
    print("  ‚úÖ Leg√≠vel por humanos (Excel, Bloco de Notas)")
    print("  ‚úÖ F√°cil de debugar e inspecionar")
    print("  ‚úÖ Universal (suportado por todas as ferramentas)")
    print("  ‚úÖ Row-based (otimizado para transa√ß√µes)")
    print("  ‚ùå Arquivo maior (sem compress√£o)")
    print("  ‚ùå Leitura mais lenta em grandes volumes")
    print("  ‚ùå N√£o preserva tipos de dados (tudo √© texto)")
    
    print("\nüìä Parquet (Apache Parquet):")
    print("  ‚úÖ Arquivo menor (compress√£o eficiente)")
    print("  ‚úÖ Leitura muito mais r√°pida (columnar)")
    print("  ‚úÖ Mant√©m schema (tipos de dados)")
    print("  ‚úÖ Ideal para Big Data e Analytics")
    print("  ‚úÖ Columnar (otimizado para consultas anal√≠ticas)")
    print("  ‚ùå N√£o √© leg√≠vel diretamente por humanos")
    print("  ‚ùå Requer ferramentas especiais (Pandas, Spark)")
    
    print("\n" + "=" * 60)
    print("QUANDO USAR CADA UM?")
    print("=" * 60)
    print("\nüìÑ Use CSV quando:")
    print("  - Dados pequenos ou m√©dios (< 100MB)")
    print("  - Precisa ser leg√≠vel por humanos")
    print("  - Integra√ß√£o com Excel/ferramentas de neg√≥cio")
    print("  - Debugging e inspe√ß√£o manual")
    
    print("\nüìä Use Parquet quando:")
    print("  - Grandes volumes de dados (> 100MB)")
    print("  - Performance √© cr√≠tica")
    print("  - Data Lakes e Data Warehouses")
    print("  - An√°lises anal√≠ticas (OLAP)")
    print("  - Processamento com Spark/PySpark")
else:
    print("‚ö†Ô∏è Arquivos n√£o encontrados. Execute as c√©lulas anteriores primeiro.")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 10. Resumo do Pipeline
# MAGIC
# MAGIC Este pipeline completo realiza:
# MAGIC
# MAGIC 1. ‚úÖ **Extra√ß√£o**: Busca dados da API Coinbase
# MAGIC 2. ‚úÖ **Transforma√ß√£o**: Trata e renomeia colunas, adiciona timestamp
# MAGIC 3. ‚úÖ **Carga**: Salva em m√∫ltiplos formatos:
# MAGIC    - JSON (atualiza√ß√£o incremental)
# MAGIC    - Parquet (otimizado para Big Data)
# MAGIC    - CSV (leg√≠vel por humanos)
# MAGIC 4. ‚úÖ **Armazenamento**: Utiliza Volumes do Databricks para organiza√ß√£o
# MAGIC
# MAGIC **Pr√≥ximos passos**: Criar dashboard e agente de IA para an√°lise dos dados!

# COMMAND ----------


