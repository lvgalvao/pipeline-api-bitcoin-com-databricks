# Databricks notebook source
# MAGIC %md
# MAGIC # üí∞ Extra√ß√£o de Dados Bitcoin - API Coinbase
# MAGIC
# MAGIC Este notebook demonstra como:
# MAGIC - Extrair dados da API da Coinbase
# MAGIC - Converter USD para BRL usando API de economia
# MAGIC - Salvar dados em JSON, CSV e Parquet usando PySpark
# MAGIC - Salvar como **Delta Table** no Databricks
# MAGIC - Converter Delta Table para DataFrame
# MAGIC - Trabalhar com **Unity Catalog** (Catalog, Schema, Volume)
# MAGIC
# MAGIC ---

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. Importando Bibliotecas Necess√°rias

# COMMAND ----------

# TODO: Importar as bibliotecas necess√°rias
# - requests (para fazer requisi√ß√µes HTTP)
# - json (para trabalhar com JSON)
# - datetime (para trabalhar com timestamps)

# Digite o c√≥digo aqui:


# No Databricks, o Spark j√° est√° dispon√≠vel como 'spark', n√£o precisa criar SparkSession

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. Extraindo e Transformando Dados

# COMMAND ----------

# TODO: Criar fun√ß√£o para extrair dados do Bitcoin da API Coinbase
# URL: https://api.coinbase.com/v2/prices/spot
# Retornar: resposta.json()

def extrair_dados_bitcoin():
    """Extrai o JSON completo da API da Coinbase."""
    # Digite o c√≥digo aqui:
    pass

# COMMAND ----------

# TODO: Criar fun√ß√£o para extrair cota√ß√£o USD-BRL da API CurrencyFreaks
# URL: https://api.currencyfreaks.com/v2.0/rates/latest?apikey={api_key}
# API Key: 'bf8ddcbf744d4e7897e836941c32e39f'
# Retornar: resposta.json()

def extrair_cotacao_usd_brl():
    """Extrai a cota√ß√£o USD-BRL da API CurrencyFreaks."""
    # Digite o c√≥digo aqui:
    pass

# COMMAND ----------

# TODO: Criar fun√ß√£o para tratar os dados do Bitcoin
# Par√¢metros: dados_json, taxa_usd_brl
# Extrair: valor_usd, criptomoeda, moeda_original
# Calcular: valor_brl = valor_usd * taxa_usd_brl
# Adicionar: timestamp usando datetime.now()
# Retornar: lista com dicion√°rio contendo os dados tratados

def tratar_dados_bitcoin(dados_json, taxa_usd_brl):
    """Transforma os dados brutos da API, renomeia colunas, adiciona timestamp e converte para BRL."""
    # Digite o c√≥digo aqui:
    pass

# COMMAND ----------

# TODO: Executar as fun√ß√µes para extrair e transformar os dados
# 1. Chamar extrair_dados_bitcoin() e armazenar em Dados_bitcoin
# 2. Chamar extrair_cotacao_usd_brl() e armazenar em dados_cotacao
# 3. Extrair a taxa de convers√£o: dados_cotacao['rates']['BRL']
# 4. Chamar tratar_dados_bitcoin() com os dados e a taxa

# Digite o c√≥digo aqui:


# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Configurando Unity Catalog

# COMMAND ----------

# TODO: Criar o Catalog no Unity Catalog
# Nome: pipeline_api_bitcoin
# Coment√°rio: 'Cat√°logo de demonstra√ß√£o criado para o workshop de pipeline_api_bitcoin'

# MAGIC %sql
# MAGIC -- Digite o SQL aqui:


# COMMAND ----------

# TODO: Criar o Schema lakehouse
# Nome: pipeline_api_bitcoin.lakehouse
# Coment√°rio: 'Schema Lakehouse para salvar dados processados'

# MAGIC %sql
# MAGIC -- Digite o SQL aqui:


# COMMAND ----------

# TODO: Criar o Volume para arquivos brutos
# Nome: pipeline_api_bitcoin.lakehouse.raw_files
# Coment√°rio: 'Volume para arquivos brutos de ingest√£o inicial'

# MAGIC %sql
# MAGIC -- Digite o SQL aqui:


# COMMAND ----------

# TODO: Criar o Schema bitcoin_data
# Nome: pipeline_api_bitcoin.bitcoin_data
# Coment√°rio: 'Schema para armazenar dados de Bitcoin processados'

# MAGIC %sql
# MAGIC -- Digite o SQL aqui:


# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. Criando Spark DataFrame
# MAGIC
# MAGIC No Databricks, o objeto `spark` (SparkSession) j√° est√° dispon√≠vel automaticamente.

# COMMAND ----------

# TODO: Criar Spark DataFrame a partir dos dados tratados
# Usar: spark.createDataFrame(dados_bitcoin_tratado)
# Mostrar o schema usando: df.printSchema()

# Digite o c√≥digo aqui:


# COMMAND ----------

# MAGIC %md
# MAGIC ## 5. Salvando em JSON usando PySpark

# COMMAND ----------

# TODO: Salvar DataFrame como JSON
# 1. Pegar o timestamp do primeiro registro: dados_bitcoin_tratado[0]["timestamp"]
# 2. Converter para formato de arquivo: .strftime("%Y%m%d_%H%M%S_%f")
# 3. Criar caminho: f"/Volumes/pipeline_api_bitcoin/lakehouse/raw_files/bitcoin_{ts}.json"
# 4. Salvar usando: df.write.mode("overwrite").json(json_path)

# Digite o c√≥digo aqui:


# COMMAND ----------

# MAGIC %md
# MAGIC ## 6. Salvando como CSV usando PySpark
# MAGIC
# MAGIC ### üìÑ O que √© CSV?
# MAGIC
# MAGIC **CSV (Comma-Separated Values)** √© um formato de arquivo **texto** simples e universal:
# MAGIC
# MAGIC **Caracter√≠sticas T√©cnicas:**
# MAGIC - ‚úÖ **Formato texto**: Arquivo leg√≠vel por humanos (pode abrir no Bloco de Notas)
# MAGIC - ‚úÖ **Row-based**: Armazena dados por linha (otimizado para transa√ß√µes OLTP)
# MAGIC - ‚úÖ **Delimitado por v√≠rgulas**: Valores separados por v√≠rgula (ou ponto-e-v√≠rgula)
# MAGIC - ‚úÖ **Universal**: Suportado por praticamente todas as ferramentas (Excel, Python, R, SQL, etc.)
# MAGIC - ‚úÖ **Simples**: F√°cil de entender, debugar e inspecionar manualmente
# MAGIC
# MAGIC **Vantagens:**
# MAGIC - ‚úÖ **Legibilidade**: Pode ser aberto no Excel, Bloco de Notas, Google Sheets
# MAGIC - ‚úÖ **Compatibilidade**: Funciona com qualquer ferramenta de dados
# MAGIC - ‚úÖ **Debugging**: F√°cil de inspecionar e corrigir manualmente
# MAGIC - ‚úÖ **Portabilidade**: Pode ser compartilhado facilmente
# MAGIC
# MAGIC **Desvantagens:**
# MAGIC - ‚ùå **Maior tamanho**: Arquivos maiores que Parquet (sem compress√£o nativa)
# MAGIC - ‚ùå **Mais lento**: Leitura e escrita mais lentas em grandes volumes
# MAGIC - ‚ùå **Sem schema**: N√£o preserva tipos de dados automaticamente (tudo √© texto)
# MAGIC - ‚ùå **Sem compress√£o**: Arquivos ocupam mais espa√ßo em disco
# MAGIC - ‚ùå **Limita√ß√µes**: N√£o suporta tipos complexos (arrays, objetos aninhados)
# MAGIC
# MAGIC **Quando usar CSV?**
# MAGIC - Dados pequenos ou m√©dios (< 100MB)
# MAGIC - Quando precisa ser leg√≠vel por humanos
# MAGIC - Integra√ß√£o com Excel ou ferramentas de neg√≥cio
# MAGIC - Debugging e inspe√ß√£o manual dos dados
# MAGIC - Exporta√ß√£o para sistemas que n√£o suportam Parquet
# MAGIC - Prototipagem r√°pida

# COMMAND ----------

# TODO: Salvar DataFrame como CSV
# 1. Criar caminho: f"/Volumes/pipeline_api_bitcoin/lakehouse/raw_files/bitcoin_{ts}.csv"
# 2. Salvar usando: df.write.mode("overwrite").option("header", "true").csv(csv_path)

# Digite o c√≥digo aqui:


# COMMAND ----------

# MAGIC %md
# MAGIC ## 7. Salvando como Parquet usando PySpark
# MAGIC
# MAGIC ### üìä O que √© Parquet?
# MAGIC
# MAGIC **Parquet** √© um formato de arquivo **bin√°rio e columnar** otimizado para Big Data:
# MAGIC
# MAGIC **Caracter√≠sticas T√©cnicas:**
# MAGIC - ‚úÖ **Formato bin√°rio**: Arquivo n√£o leg√≠vel por humanos (otimizado para m√°quinas)
# MAGIC - ‚úÖ **Columnar**: Armazena dados por coluna, n√£o por linha (otimizado para analytics OLAP)
# MAGIC - ‚úÖ **Compress√£o nativa**: Usa algoritmos de compress√£o eficientes (Snappy, Gzip, LZ4)
# MAGIC - ‚úÖ **Schema embutido**: Mant√©m informa√ß√µes sobre tipos de dados automaticamente
# MAGIC - ‚úÖ **Predicate pushdown**: Permite ler apenas colunas necess√°rias
# MAGIC - ‚úÖ **Estat√≠sticas**: Inclui estat√≠sticas (min, max, null count) por coluna
# MAGIC
# MAGIC **Vantagens:**
# MAGIC - ‚úÖ **Compress√£o**: Arquivos muito menores que CSV (at√© 90% de economia)
# MAGIC - ‚úÖ **Performance**: Leitura r√°pida, especialmente para consultas anal√≠ticas
# MAGIC - ‚úÖ **Big Data**: Ideal para processar grandes volumes de dados (terabytes/petabytes)
# MAGIC - ‚úÖ **Schema**: Preserva tipos de dados (int, float, string, date, etc.)
# MAGIC - ‚úÖ **Columnar**: Consultas que precisam de poucas colunas s√£o muito r√°pidas
# MAGIC - ‚úÖ **Efici√™ncia**: Menor uso de I/O e mem√≥ria
# MAGIC
# MAGIC **Desvantagens:**
# MAGIC - ‚ùå **N√£o √© leg√≠vel por humanos**: Precisa de ferramentas especiais (Pandas, Spark, etc.)
# MAGIC - ‚ùå **Overhead**: Para dados muito pequenos, o overhead pode n√£o valer a pena
# MAGIC - ‚ùå **Escrita mais lenta**: A compress√£o e organiza√ß√£o columnar tornam a escrita mais lenta
# MAGIC
# MAGIC **Quando usar Parquet?**
# MAGIC - Grandes volumes de dados (> 100MB)
# MAGIC - Data Lakes e Data Warehouses
# MAGIC - Quando performance √© cr√≠tica
# MAGIC - An√°lises anal√≠ticas (OLAP)
# MAGIC - Processamento com Spark/PySpark
# MAGIC - Quando economia de espa√ßo √© importante
# MAGIC - Consultas que acessam poucas colunas de muitas linhas

# COMMAND ----------

# TODO: Salvar DataFrame como Parquet
# 1. Criar caminho: f"/Volumes/pipeline_api_bitcoin/lakehouse/raw_files/bitcoin_{ts}.parquet"
# 2. Salvar usando: df.write.mode("overwrite").parquet(parquet_path)

# Digite o c√≥digo aqui:


# COMMAND ----------

# MAGIC %md
# MAGIC ## 8. Salvando como Delta Table
# MAGIC
# MAGIC ### üéØ O que √© Delta Table?
# MAGIC
# MAGIC **Delta Lake** √© uma camada de armazenamento open-source que traz **ACID transactions** e **time travel** para Data Lakes. Delta Tables s√£o tabelas que usam o formato Delta.
# MAGIC
# MAGIC **Caracter√≠sticas T√©cnicas:**
# MAGIC - ‚úÖ **ACID Transactions**: Garante consist√™ncia dos dados (Atomicity, Consistency, Isolation, Durability)
# MAGIC - ‚úÖ **Time Travel**: Permite acessar vers√µes anteriores dos dados
# MAGIC - ‚úÖ **Schema Enforcement**: Valida e garante que os dados seguem o schema definido
# MAGIC - ‚úÖ **Upsert/Merge**: Suporta opera√ß√µes de atualiza√ß√£o e merge eficientes
# MAGIC - ‚úÖ **Baseado em Parquet**: Usa Parquet como formato f√≠sico (herda todas as vantagens)
# MAGIC - ‚úÖ **Transaction Log**: Mant√©m um log de todas as transa√ß√µes (Delta Log)
# MAGIC - ‚úÖ **Optimize & Z-Order**: Ferramentas para otimizar performance
# MAGIC
# MAGIC **Vantagens sobre Parquet simples:**
# MAGIC - ‚úÖ **Transa√ß√µes ACID**: M√∫ltiplas escritas simult√¢neas sem corrup√ß√£o
# MAGIC - ‚úÖ **Time Travel**: Acesse vers√µes hist√≥ricas dos dados
# MAGIC - ‚úÖ **Schema Evolution**: Permite evoluir o schema ao longo do tempo
# MAGIC - ‚úÖ **Delete/Update**: Suporta opera√ß√µes de atualiza√ß√£o e dele√ß√£o
# MAGIC - ‚úÖ **Auditoria**: Hist√≥rico completo de mudan√ßas
# MAGIC - ‚úÖ **Performance**: Otimiza√ß√µes autom√°ticas (compacta√ß√£o, indexa√ß√£o)
# MAGIC
# MAGIC **Quando usar Delta Table?**
# MAGIC - Quando precisa de transa√ß√µes ACID
# MAGIC - Quando precisa de time travel (auditoria, rollback)
# MAGIC - Quando precisa fazer updates/deletes em dados hist√≥ricos
# MAGIC - Data Warehouses modernos (Lakehouse)
# MAGIC - Pipelines que precisam de garantias de consist√™ncia
# MAGIC - Quando m√∫ltiplos processos escrevem na mesma tabela

# COMMAND ----------

# TODO: Salvar DataFrame como Delta Table
# 1. Definir caminho: "pipeline_api_bitcoin.bitcoin_data.bitcoin_data"
# 2. Salvar usando: df.write.format("delta").mode("append").option("mergeSchema", "true").saveAsTable(delta_table_path)

# Digite o c√≥digo aqui:


# COMMAND ----------

# MAGIC %md
# MAGIC ## 9. Convertendo Delta Table para DataFrame

# COMMAND ----------

# TODO: Ler a Delta Table como Spark DataFrame
# Usar: spark.read.table(delta_table_path)
# Mostrar schema: df_delta.printSchema()
# Mostrar dados: df_delta.show(truncate=False)

# Digite o c√≥digo aqui:


# COMMAND ----------

# MAGIC %md
# MAGIC ### Consultando Delta Table com SQL

# COMMAND ----------

# TODO: Consultar a Delta Table usando SQL
# Selecionar todas as colunas, ordenar por timestamp DESC, limitar a 10 registros

# MAGIC %sql
# MAGIC -- Digite o SQL aqui:


# COMMAND ----------

# MAGIC %md
# MAGIC ### Verificando hist√≥rico da Delta Table (Time Travel)

# COMMAND ----------

# TODO: Verificar o hist√≥rico da Delta Table
# Usar: DESCRIBE HISTORY

# MAGIC %sql
# MAGIC -- Digite o SQL aqui:


# COMMAND ----------

# MAGIC %md
# MAGIC ## 11. Resumo do Pipeline
# MAGIC
# MAGIC Este pipeline completo realiza:
# MAGIC
# MAGIC 1. ‚úÖ **Extra√ß√£o**: Busca dados da API Coinbase e cota√ß√£o USD-BRL
# MAGIC 2. ‚úÖ **Transforma√ß√£o**: Trata dados, converte para BRL e adiciona timestamp
# MAGIC 3. ‚úÖ **Infraestrutura**: Cria Catalog e Schema Lakehouse no Unity Catalog
# MAGIC 4. ‚úÖ **Carga em m√∫ltiplos formatos usando PySpark**:
# MAGIC    - **JSON**: Formato texto leg√≠vel, ideal para dados brutos
# MAGIC    - **CSV**: Formato texto universal, leg√≠vel por humanos (salvo com PySpark)
# MAGIC    - **Parquet**: Formato bin√°rio columnar, otimizado para Big Data (salvo com PySpark)
# MAGIC    - **Delta Table**: Formato com ACID transactions e time travel
# MAGIC 5. ‚úÖ **Convers√£o**: Delta Table ‚Üí Spark DataFrame
# MAGIC
# MAGIC **Compara√ß√£o de Formatos:**
# MAGIC - üìÑ **CSV**: Leg√≠vel, maior, mais lento ‚Üí Use para debugging e dados pequenos
# MAGIC - üìä **Parquet**: Bin√°rio, menor, mais r√°pido ‚Üí Use para Big Data e analytics
# MAGIC - üéØ **Delta Table**: Parquet + ACID + Time Travel ‚Üí Use para Data Warehouses e pipelines cr√≠ticos
# MAGIC
# MAGIC **Pr√≥ximos passos**: Criar dashboard e agente de IA para an√°lise dos dados!

