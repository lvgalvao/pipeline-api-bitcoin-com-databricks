# Databricks notebook source
# MAGIC %md
# MAGIC # üí∞ Extra√ß√£o de Dados Bitcoin - API Coinbase
# MAGIC
# MAGIC Este notebook demonstra como:
# MAGIC - Extrair dados da API da Coinbase
# MAGIC - Adicionar timestamp aos dados
# MAGIC - Salvar dados em JSON, CSV e Parquet
# MAGIC - Salvar como **Delta Table** no Databricks
# MAGIC - Converter Delta Table para DataFrame
# MAGIC - Trabalhar com **Unity Catalog** (Catalog, Schema, Volume)
# MAGIC
# MAGIC ---

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. Importando Bibliotecas Necess√°rias

# COMMAND ----------

import requests
import json
from datetime import datetime
import pandas as pd
import os

# Para trabalhar com Spark e Delta Tables
from pyspark.sql import SparkSession

print("‚úÖ Bibliotecas importadas com sucesso!")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. Configurando URLs e Par√¢metros da API

# COMMAND ----------

# URL da API Coinbase para obter o pre√ßo spot
url = 'https://api.coinbase.com/v2/prices/spot'

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Extraindo Dados da API Coinbase

# COMMAND ----------

def extrair_dados_bitcoin():
    """Extrai o JSON completo da API da Coinbase."""
    url = 'https://api.coinbase.com/v2/prices/spot'
    resposta = requests.get(url)
    return resposta.json()

# Extraindo dados
dados_json = extrair_dados_bitcoin()
print("‚úÖ Dados extra√≠dos com sucesso!")
print("\nResposta da API:")
print(json.dumps(dados_json, indent=2))

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. Extraindo Cota√ß√£o USD-BRL da API de Economia

# COMMAND ----------

def extrair_cotacao_usd_brl():
    """Extrai a cota√ß√£o USD-BRL da API AwesomeAPI."""
    url = 'https://economia.awesomeapi.com.br/json/last/USD-BRL'
    resposta = requests.get(url)
    return resposta.json()

# Extraindo cota√ß√£o USD-BRL
cotacao_json = extrair_cotacao_usd_brl()
print("‚úÖ Cota√ß√£o USD-BRL extra√≠da com sucesso!")
print("\nResposta da API de Economia:")
print(json.dumps(cotacao_json, indent=2))

# Extraindo a taxa de convers√£o (bid = taxa de compra)
taxa_usd_brl = float(cotacao_json['USDBRL']['bid'])
print(f"\nüí± Taxa de convers√£o USD-BRL: R$ {taxa_usd_brl:.4f}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5. Tratando Dados, Adicionando Timestamp e Convertendo para BRL

# COMMAND ----------

def tratar_dados_bitcoin(dados_json, taxa_usd_brl):
    """Transforma os dados brutos da API, renomeia colunas, adiciona timestamp e converte para BRL."""
    valor_usd = float(dados_json['data']['amount'])
    criptomoeda = dados_json['data']['base']
    moeda_original = dados_json['data']['currency']
    
    # Convertendo de USD para BRL
    valor_brl = valor_usd * taxa_usd_brl
    
    # Adicionando timestamp
    timestamp = datetime.now().isoformat()
    timestamp_readable = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    dados_tratados = [{
        "valor_usd": valor_usd,
        "valor_brl": valor_brl,
        "criptomoeda": criptomoeda,
        "moeda_original": moeda_original,
        "taxa_conversao_usd_brl": taxa_usd_brl,
        "timestamp": timestamp,
        "timestamp_readable": timestamp_readable
    }]
    
    return dados_tratados

# Tratando os dados e convertendo para BRL
dados_bitcoin = tratar_dados_bitcoin(dados_json, taxa_usd_brl)
print("‚úÖ Dados tratados e convertidos para BRL com sucesso!")
print("\nDados processados:")
print(json.dumps(dados_bitcoin, indent=2, ensure_ascii=False))
print(f"\nüí∞ Bitcoin em USD: ${dados_bitcoin[0]['valor_usd']:,.2f}")
print(f"üí∞ Bitcoin em BRL: R$ {dados_bitcoin[0]['valor_brl']:,.2f}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 6. Configurando nosso Cat√°logo
# MAGIC
# MAGIC ### üìÅ O que √© um Cat√°logo no Databricks?
# MAGIC
# MAGIC **Cat√°logos** s√£o o n√≠vel mais alto de organiza√ß√£o de dados no Databricks, dentro do **Unity Catalog**. Eles funcionam como um *container l√≥gico* que agrupa schemas, tabelas, views e volumes, garantindo governan√ßa, seguran√ßa e organiza√ß√£o em escala.
# MAGIC
# MAGIC Um Cat√°logo permite:
# MAGIC - ‚úÖ **Governan√ßa centralizada**: Controle de acesso unificado para dados, arquivos e assets
# MAGIC - ‚úÖ **Organiza√ß√£o l√≥gica**: Separa√ß√£o clara por dom√≠nio, time ou finalidade (ex: `main`, `dev`, `analytics`)
# MAGIC - ‚úÖ **Seguran√ßa**: Permiss√µes granulares por cat√°logo, schema e objeto
# MAGIC - ‚úÖ **Padroniza√ß√£o**: Base para boas pr√°ticas de arquitetura de dados
# MAGIC
# MAGIC ### üß± Hierarquia no Databricks
# MAGIC
# MAGIC A organiza√ß√£o segue a hierarquia:
# MAGIC
# MAGIC `Catalog ‚Üí Schema ‚Üí Tabelas / Views / Volumes`

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE CATALOG IF NOT EXISTS pipeline_api_bitcoin
# MAGIC COMMENT 'Cat√°logo de demonstra√ß√£o criado para o workshop de pipeline_api_bitcoin';

# COMMAND ----------

# MAGIC %md
# MAGIC ## 7. Criar um SCHEMA (database) no cat√°logo

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE SCHEMA IF NOT EXISTS pipeline_api_bitcoin.datalake
# MAGIC COMMENT 'Schema Datalake para salvar dados brutos e heterog√™neos';

# COMMAND ----------

# MAGIC %md
# MAGIC ## 8. Criar um Volume no cat√°logo

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE VOLUME IF NOT EXISTS pipeline_api_bitcoin.datalake.raw_files
# MAGIC COMMENT 'Volume para arquivos brutos de ingest√£o inicial';

# COMMAND ----------

# MAGIC %md
# MAGIC ## 9. Salvando em JSON

# COMMAND ----------

# Pega o timestamp do pr√≥prio evento
event_ts = dados_bitcoin[0]["timestamp"]

# Converte para formato seguro para nome de arquivo
ts = datetime.fromisoformat(event_ts).strftime("%Y%m%d_%H%M%S_%f")

path = (
    f"/Volumes/pipeline_api_bitcoin/datalake/raw_files/"
    f"bitcoin_{ts}.json"
)

with open(path, "w", encoding='utf-8') as f:
    json.dump(dados_bitcoin, f, indent=2, ensure_ascii=False)

print(f"‚úÖ JSON salvo em: {path}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 10. Salvando como CSV
# MAGIC
# MAGIC ### üìÑ O que √© CSV?
# MAGIC
# MAGIC **CSV (Comma-Separated Values)** √© um formato de arquivo **texto** simples e universal:
# MAGIC
# MAGIC **Caracter√≠sticas T√©cnicas:**
# MAGIC - ‚úÖ **Formato texto**: Arquivo leg√≠vel por humanos (pode abrir no Bloco de Notas)
# MAGIC - ‚úÖ **Row-based**: Armazena dados por linha (otimizado para transa√ß√µes OLTP)
# MAGIC - ‚úÖ **Delimitado por v√≠rgulas**: Valores separados por v√≠rgula (ou ponto-e-v√≠rgula)
# MAGIC - ‚úÖ **Universal**: Suportado por praticamente todas as ferramentas (Excel, Python, R, SQL, etc.)
# MAGIC - ‚úÖ **Simples**: F√°cil de entender, debugar e inspecionar manualmente
# MAGIC
# MAGIC **Vantagens:**
# MAGIC - ‚úÖ **Legibilidade**: Pode ser aberto no Excel, Bloco de Notas, Google Sheets
# MAGIC - ‚úÖ **Compatibilidade**: Funciona com qualquer ferramenta de dados
# MAGIC - ‚úÖ **Debugging**: F√°cil de inspecionar e corrigir manualmente
# MAGIC - ‚úÖ **Portabilidade**: Pode ser compartilhado facilmente
# MAGIC
# MAGIC **Desvantagens:**
# MAGIC - ‚ùå **Maior tamanho**: Arquivos maiores que Parquet (sem compress√£o nativa)
# MAGIC - ‚ùå **Mais lento**: Leitura e escrita mais lentas em grandes volumes
# MAGIC - ‚ùå **Sem schema**: N√£o preserva tipos de dados automaticamente (tudo √© texto)
# MAGIC - ‚ùå **Sem compress√£o**: Arquivos ocupam mais espa√ßo em disco
# MAGIC - ‚ùå **Limita√ß√µes**: N√£o suporta tipos complexos (arrays, objetos aninhados)
# MAGIC
# MAGIC **Quando usar CSV?**
# MAGIC - Dados pequenos ou m√©dios (< 100MB)
# MAGIC - Quando precisa ser leg√≠vel por humanos
# MAGIC - Integra√ß√£o com Excel ou ferramentas de neg√≥cio
# MAGIC - Debugging e inspe√ß√£o manual dos dados
# MAGIC - Exporta√ß√£o para sistemas que n√£o suportam Parquet
# MAGIC - Prototipagem r√°pida

# COMMAND ----------

# Criar DataFrame do Pandas
df_csv = pd.DataFrame(dados_bitcoin)

# Caminho do arquivo CSV no volume
csv_path = f"/Volumes/pipeline_api_bitcoin/datalake/raw_files/bitcoin_{ts}.csv"

# Salvar como CSV
df_csv.to_csv(csv_path, index=False, encoding='utf-8')

# Verificar tamanho do arquivo
tamanho_csv = os.path.getsize(csv_path)

print(f"‚úÖ CSV salvo em: {csv_path}")
print(f"üì¶ Tamanho: {tamanho_csv:,} bytes ({tamanho_csv/1024:.2f} KB)")
print(f"üìä Total de registros: {len(df_csv)}")

# Mostrar dados
print("\nüìÑ Conte√∫do do CSV:")
print(df_csv)

# COMMAND ----------

# MAGIC %md
# MAGIC ## 11. Salvando como Parquet
# MAGIC
# MAGIC ### üìä O que √© Parquet?
# MAGIC
# MAGIC **Parquet** √© um formato de arquivo **bin√°rio e columnar** otimizado para Big Data:
# MAGIC
# MAGIC **Caracter√≠sticas T√©cnicas:**
# MAGIC - ‚úÖ **Formato bin√°rio**: Arquivo n√£o leg√≠vel por humanos (otimizado para m√°quinas)
# MAGIC - ‚úÖ **Columnar**: Armazena dados por coluna, n√£o por linha (otimizado para analytics OLAP)
# MAGIC - ‚úÖ **Compress√£o nativa**: Usa algoritmos de compress√£o eficientes (Snappy, Gzip, LZ4)
# MAGIC - ‚úÖ **Schema embutido**: Mant√©m informa√ß√µes sobre tipos de dados automaticamente
# MAGIC - ‚úÖ **Predicate pushdown**: Permite ler apenas colunas necess√°rias
# MAGIC - ‚úÖ **Estat√≠sticas**: Inclui estat√≠sticas (min, max, null count) por coluna
# MAGIC
# MAGIC **Vantagens:**
# MAGIC - ‚úÖ **Compress√£o**: Arquivos muito menores que CSV (at√© 90% de economia)
# MAGIC - ‚úÖ **Performance**: Leitura r√°pida, especialmente para consultas anal√≠ticas
# MAGIC - ‚úÖ **Big Data**: Ideal para processar grandes volumes de dados (terabytes/petabytes)
# MAGIC - ‚úÖ **Schema**: Preserva tipos de dados (int, float, string, date, etc.)
# MAGIC - ‚úÖ **Columnar**: Consultas que precisam de poucas colunas s√£o muito r√°pidas
# MAGIC - ‚úÖ **Efici√™ncia**: Menor uso de I/O e mem√≥ria
# MAGIC
# MAGIC **Desvantagens:**
# MAGIC - ‚ùå **N√£o √© leg√≠vel por humanos**: Precisa de ferramentas especiais (Pandas, Spark, etc.)
# MAGIC - ‚ùå **Overhead**: Para dados muito pequenos, o overhead pode n√£o valer a pena
# MAGIC - ‚ùå **Escrita mais lenta**: A compress√£o e organiza√ß√£o columnar tornam a escrita mais lenta
# MAGIC
# MAGIC **Quando usar Parquet?**
# MAGIC - Grandes volumes de dados (> 100MB)
# MAGIC - Data Lakes e Data Warehouses
# MAGIC - Quando performance √© cr√≠tica
# MAGIC - An√°lises anal√≠ticas (OLAP)
# MAGIC - Processamento com Spark/PySpark
# MAGIC - Quando economia de espa√ßo √© importante
# MAGIC - Consultas que acessam poucas colunas de muitas linhas

# COMMAND ----------

# Criar DataFrame do Pandas
df_parquet = pd.DataFrame(dados_bitcoin)

# Caminho do arquivo Parquet no volume
parquet_path = f"/Volumes/pipeline_api_bitcoin/datalake/raw_files/bitcoin_{ts}.parquet"

# Salvar como Parquet
df_parquet.to_parquet(parquet_path, index=False, engine='pyarrow')

# Verificar tamanho do arquivo
tamanho_parquet = os.path.getsize(parquet_path)

print(f"‚úÖ Parquet salvo em: {parquet_path}")
print(f"üì¶ Tamanho: {tamanho_parquet:,} bytes ({tamanho_parquet/1024:.2f} KB)")
print(f"üìä Total de registros: {len(df_parquet)}")

# Mostrar dados
print("\nüìä Conte√∫do do Parquet:")
print(df_parquet)

# COMMAND ----------

# MAGIC %md
# MAGIC ## 12. Compara√ß√£o: CSV vs Parquet

# COMMAND ----------

if os.path.exists(csv_path) and os.path.exists(parquet_path):
    tamanho_csv = os.path.getsize(csv_path)
    tamanho_parquet = os.path.getsize(parquet_path)
    
    print("=" * 70)
    print("COMPARA√á√ÉO: CSV vs PARQUET")
    print("=" * 70)
    
    print("\nüìÑ CSV:")
    print(f"   Tamanho: {tamanho_csv:,} bytes ({tamanho_csv/1024:.2f} KB)")
    print("   Tipo: Texto (Row-based)")
    print("   Leg√≠vel: Sim (Excel, Bloco de Notas)")
    
    print("\nüìä Parquet:")
    print(f"   Tamanho: {tamanho_parquet:,} bytes ({tamanho_parquet/1024:.2f} KB)")
    print("   Tipo: Bin√°rio (Columnar)")
    print("   Leg√≠vel: N√£o (requer ferramentas especiais)")
    
    if tamanho_csv > 0:
        economia = ((tamanho_csv - tamanho_parquet) / tamanho_csv) * 100
        print(f"\nüíæ Economia com Parquet: {economia:.1f}%")
        print(f"   (CSV √© {tamanho_csv/tamanho_parquet:.1f}x maior que Parquet)")
    
    print("\n" + "=" * 70)
    print("RESUMO")
    print("=" * 70)
    print("\nüìÑ CSV: Leg√≠vel, maior, mais lento ‚Üí Use para dados pequenos e debugging")
    print("üìä Parquet: Bin√°rio, menor, mais r√°pido ‚Üí Use para Big Data e analytics")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 13. Salvando como Delta Table
# MAGIC
# MAGIC ### üéØ O que √© Delta Table?
# MAGIC
# MAGIC **Delta Lake** √© uma camada de armazenamento open-source que traz **ACID transactions** e **time travel** para Data Lakes. Delta Tables s√£o tabelas que usam o formato Delta.
# MAGIC
# MAGIC **Caracter√≠sticas T√©cnicas:**
# MAGIC - ‚úÖ **ACID Transactions**: Garante consist√™ncia dos dados (Atomicity, Consistency, Isolation, Durability)
# MAGIC - ‚úÖ **Time Travel**: Permite acessar vers√µes anteriores dos dados
# MAGIC - ‚úÖ **Schema Enforcement**: Valida e garante que os dados seguem o schema definido
# MAGIC - ‚úÖ **Upsert/Merge**: Suporta opera√ß√µes de atualiza√ß√£o e merge eficientes
# MAGIC - ‚úÖ **Baseado em Parquet**: Usa Parquet como formato f√≠sico (herda todas as vantagens)
# MAGIC - ‚úÖ **Transaction Log**: Mant√©m um log de todas as transa√ß√µes (Delta Log)
# MAGIC - ‚úÖ **Optimize & Z-Order**: Ferramentas para otimizar performance
# MAGIC
# MAGIC **Vantagens sobre Parquet simples:**
# MAGIC - ‚úÖ **Transa√ß√µes ACID**: M√∫ltiplas escritas simult√¢neas sem corrup√ß√£o
# MAGIC - ‚úÖ **Time Travel**: Acesse vers√µes hist√≥ricas dos dados
# MAGIC - ‚úÖ **Schema Evolution**: Permite evoluir o schema ao longo do tempo
# MAGIC - ‚úÖ **Delete/Update**: Suporta opera√ß√µes de atualiza√ß√£o e dele√ß√£o
# MAGIC - ‚úÖ **Auditoria**: Hist√≥rico completo de mudan√ßas
# MAGIC - ‚úÖ **Performance**: Otimiza√ß√µes autom√°ticas (compacta√ß√£o, indexa√ß√£o)
# MAGIC
# MAGIC **Quando usar Delta Table?**
# MAGIC - Quando precisa de transa√ß√µes ACID
# MAGIC - Quando precisa de time travel (auditoria, rollback)
# MAGIC - Quando precisa fazer updates/deletes em dados hist√≥ricos
# MAGIC - Data Warehouses modernos (Lakehouse)
# MAGIC - Pipelines que precisam de garantias de consist√™ncia
# MAGIC - Quando m√∫ltiplos processos escrevem na mesma tabela

# COMMAND ----------

# Criar SparkSession
spark = SparkSession.builder.appName("BitcoinPipeline").getOrCreate()

# Converter DataFrame do Pandas para Spark DataFrame
df_spark = spark.createDataFrame(df_parquet)

# Mostrar schema
print("üìä Schema do DataFrame Spark:")
df_spark.printSchema()

# Mostrar dados
print("\nüìä Dados do DataFrame Spark:")
df_spark.show()

# COMMAND ----------

# MAGIC %md
# MAGIC ### Salvando como Delta Table no Unity Catalog

# COMMAND ----------

# Caminho da tabela Delta no Unity Catalog
delta_table_path = "pipeline_api_bitcoin.datalake.bitcoin_data"

# Salvar como Delta Table (modo append se a tabela j√° existir)
df_spark.write \
    .format("delta") \
    .mode("append") \
    .option("mergeSchema", "true") \
    .saveAsTable(delta_table_path)

print(f"‚úÖ Delta Table salva em: {delta_table_path}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 14. Convertendo Delta Table para DataFrame
# MAGIC
# MAGIC Agora vamos ler a Delta Table e convert√™-la para DataFrame para an√°lise.

# COMMAND ----------

# Ler Delta Table como Spark DataFrame
df_delta = spark.read.table(delta_table_path)

print("‚úÖ Delta Table lida com sucesso!")
print(f"üìä Total de registros: {df_delta.count()}")

# Mostrar schema
print("\nüìä Schema da Delta Table:")
df_delta.printSchema()

# Mostrar dados
print("\nüìä Dados da Delta Table:")
df_delta.show(truncate=False)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Convertendo Spark DataFrame para Pandas DataFrame

# COMMAND ----------

# Converter Spark DataFrame para Pandas DataFrame
df_pandas = df_delta.toPandas()

print("‚úÖ Spark DataFrame convertido para Pandas DataFrame!")
print(f"\nüìä Tipo: {type(df_pandas)}")
print(f"üìä Shape: {df_pandas.shape}")
print("\nüìä Dados:")
print(df_pandas)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Consultando Delta Table com SQL

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM pipeline_api_bitcoin.datalake.bitcoin_data
# MAGIC ORDER BY timestamp DESC
# MAGIC LIMIT 10

# COMMAND ----------

# MAGIC %md
# MAGIC ### Verificando hist√≥rico da Delta Table (Time Travel)

# MAGIC Uma das grandes vantagens do Delta Lake √© o **Time Travel** - podemos acessar vers√µes anteriores dos dados!

# COMMAND ----------

# MAGIC %sql
# MAGIC DESCRIBE HISTORY pipeline_api_bitcoin.datalake.bitcoin_data

# COMMAND ----------

# MAGIC %md
# MAGIC ## 15. Resumo do Pipeline
# MAGIC
# MAGIC Este pipeline completo realiza:
# MAGIC
# MAGIC 1. ‚úÖ **Extra√ß√£o**: Busca dados da API Coinbase
# MAGIC 2. ‚úÖ **Transforma√ß√£o**: Trata dados e adiciona timestamp
# MAGIC 3. ‚úÖ **Infraestrutura**: Cria Catalog, Schema e Volume no Unity Catalog
# MAGIC 4. ‚úÖ **Carga em m√∫ltiplos formatos**:
# MAGIC    - **JSON**: Formato texto leg√≠vel, ideal para dados brutos
# MAGIC    - **CSV**: Formato texto universal, leg√≠vel por humanos
# MAGIC    - **Parquet**: Formato bin√°rio columnar, otimizado para Big Data
# MAGIC    - **Delta Table**: Formato com ACID transactions e time travel
# MAGIC 5. ‚úÖ **Convers√£o**: Delta Table ‚Üí Spark DataFrame ‚Üí Pandas DataFrame
# MAGIC
# MAGIC **Compara√ß√£o de Formatos:**
# MAGIC - üìÑ **CSV**: Leg√≠vel, maior, mais lento ‚Üí Use para debugging e dados pequenos
# MAGIC - üìä **Parquet**: Bin√°rio, menor, mais r√°pido ‚Üí Use para Big Data e analytics
# MAGIC - üéØ **Delta Table**: Parquet + ACID + Time Travel ‚Üí Use para Data Warehouses e pipelines cr√≠ticos
# MAGIC
# MAGIC **Pr√≥ximos passos**: Criar dashboard e agente de IA para an√°lise dos dados!

# COMMAND ----------
